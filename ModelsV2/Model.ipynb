{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bff0bf1d",
   "metadata": {},
   "source": [
    "The difference between this version of modeling and previous one is this add accident cause and want to discuss:\n",
    "- human behaviour vs car\n",
    "- car vs road design\n",
    "- road design vs human behavour\n",
    "\n",
    "Hence, this will convert the value of columns into different cause category:\n",
    "1. Distraction\n",
    "2. Driver decision (dynamic)\n",
    "3. Driver decision (static)\n",
    "4. Driver impaired\n",
    "5. Other\n",
    "\n",
    "Also add car type forfurther analyze.<br/>\n",
    "File: ComputedDataV2/ForModel/all_featuresV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274c1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "analyze_path = os.path.join(parent_dir, \"utils\")\n",
    "\n",
    "os.chdir(analyze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba53b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = ['Arial Unicode Ms']\n",
    "# plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from utils_model import eval_loop, to_tensors, model_preprocess, print_results, get_importance\n",
    "from utils import read_data, read_taiwan_specific\n",
    "from config_new import select_group, for_poly, group_translation\n",
    "from config import cause_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07b971db",
   "metadata": {},
   "outputs": [],
   "source": [
    "computeddata = 'ComputedDataV2'\n",
    "\n",
    "combined_data = read_data()\n",
    "taiwan, grid_filter = read_taiwan_specific(read_grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f780751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>geometry</th>\n",
       "      <th>num_accidents</th>\n",
       "      <th>accident_indices</th>\n",
       "      <th>GiZScore</th>\n",
       "      <th>hotspot</th>\n",
       "      <th>COUNTYNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>440</td>\n",
       "      <td>POINT (151240.028 2554496.136)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[23624]</td>\n",
       "      <td>-0.135864</td>\n",
       "      <td>Not Significant</td>\n",
       "      <td>臺南市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>469</td>\n",
       "      <td>POINT (151386.761 2553440.135)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[62700]</td>\n",
       "      <td>-0.135864</td>\n",
       "      <td>Not Significant</td>\n",
       "      <td>臺南市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>851</td>\n",
       "      <td>POINT (152616.304 2553432.088)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[223810]</td>\n",
       "      <td>-0.135864</td>\n",
       "      <td>Not Significant</td>\n",
       "      <td>臺南市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1460</td>\n",
       "      <td>POINT (153858.166 2555342.293)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[468309]</td>\n",
       "      <td>-0.135864</td>\n",
       "      <td>Not Significant</td>\n",
       "      <td>臺南市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1640</td>\n",
       "      <td>POINT (154155.684 2553805.801)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[349002]</td>\n",
       "      <td>-0.135864</td>\n",
       "      <td>Not Significant</td>\n",
       "      <td>臺南市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101074</th>\n",
       "      <td>1246118</td>\n",
       "      <td>POINT (350576.797 2767941.177)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[539920]</td>\n",
       "      <td>-0.135864</td>\n",
       "      <td>Not Significant</td>\n",
       "      <td>新北市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101075</th>\n",
       "      <td>1246142</td>\n",
       "      <td>POINT (350879.607 2767943.408)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[521212]</td>\n",
       "      <td>-0.135864</td>\n",
       "      <td>Not Significant</td>\n",
       "      <td>新北市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101076</th>\n",
       "      <td>1246151</td>\n",
       "      <td>POINT (351034.556 2767464.865)</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[624010, 648156]</td>\n",
       "      <td>-0.135864</td>\n",
       "      <td>Not Significant</td>\n",
       "      <td>新北市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101077</th>\n",
       "      <td>1246158</td>\n",
       "      <td>POINT (351189.515 2766986.323)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[359176]</td>\n",
       "      <td>-0.092907</td>\n",
       "      <td>Not Significant</td>\n",
       "      <td>新北市</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101078</th>\n",
       "      <td>1246168</td>\n",
       "      <td>POINT (351340.22 2767083.377)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[631455]</td>\n",
       "      <td>-0.092907</td>\n",
       "      <td>Not Significant</td>\n",
       "      <td>新北市</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101079 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          index                        geometry  num_accidents  \\\n",
       "0           440  POINT (151240.028 2554496.136)            1.0   \n",
       "1           469  POINT (151386.761 2553440.135)            1.0   \n",
       "2           851  POINT (152616.304 2553432.088)            1.0   \n",
       "3          1460  POINT (153858.166 2555342.293)            1.0   \n",
       "4          1640  POINT (154155.684 2553805.801)            1.0   \n",
       "...         ...                             ...            ...   \n",
       "101074  1246118  POINT (350576.797 2767941.177)            1.0   \n",
       "101075  1246142  POINT (350879.607 2767943.408)            1.0   \n",
       "101076  1246151  POINT (351034.556 2767464.865)            2.0   \n",
       "101077  1246158  POINT (351189.515 2766986.323)            1.0   \n",
       "101078  1246168   POINT (351340.22 2767083.377)            1.0   \n",
       "\n",
       "        accident_indices  GiZScore          hotspot COUNTYNAME  \n",
       "0                [23624] -0.135864  Not Significant        臺南市  \n",
       "1                [62700] -0.135864  Not Significant        臺南市  \n",
       "2               [223810] -0.135864  Not Significant        臺南市  \n",
       "3               [468309] -0.135864  Not Significant        臺南市  \n",
       "4               [349002] -0.135864  Not Significant        臺南市  \n",
       "...                  ...       ...              ...        ...  \n",
       "101074          [539920] -0.135864  Not Significant        新北市  \n",
       "101075          [521212] -0.135864  Not Significant        新北市  \n",
       "101076  [624010, 648156] -0.135864  Not Significant        新北市  \n",
       "101077          [359176] -0.092907  Not Significant        新北市  \n",
       "101078          [631455] -0.092907  Not Significant        新北市  \n",
       "\n",
       "[101079 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab09841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cause(cause):\n",
    "    for category, causes in cause_mapping.items():\n",
    "        if cause in causes:\n",
    "            return category\n",
    "    return \"Unknown\"\n",
    "\n",
    "combined_data[\"cause_group\"] = combined_data[\"肇因研判子類別名稱-主要\"].apply(map_cause)\n",
    "combined_data['cause_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb65f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_model import extract_features\n",
    "\n",
    "all_features_list = []\n",
    "\n",
    "for rows in range(grid_filter.shape[0]):\n",
    "    features = extract_features(grid_filter, combined_data, select_group, rows)\n",
    "    all_features_list.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b6a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_df = pd.concat(all_features_list, ignore_index=True)\n",
    "all_features_df.fillna(0, inplace=True)\n",
    "\n",
    "all_features_df['original_speed'] = all_features_df['速限-第1當事者_mean']\n",
    "all_features_df[['mrt_100m_count_mean', 'youbike_100m_count_mean', 'parkinglot_100m_count_mean', '速限-第1當事者_mean']] =\\\n",
    "      all_features_df[['mrt_100m_count_mean', 'youbike_100m_count_mean', 'parkinglot_100m_count_mean', '速限-第1當事者_mean']].\\\n",
    "        apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "all_features_df['hotspot'] = grid_filter['hotspot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f00ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = all_features_df['original_speed']\n",
    "all_features_df['original_speed'] = (col - col.min()) / (col.max() - col.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b94836",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_df.rename(columns={\n",
    "    'original_speed': 'original-speed',\n",
    "    'cause_group_Distraction': 'cause-group_Distraction',\n",
    "    'cause_group_Decision': 'cause-group_Decision',\n",
    "    'cause_group_Unidentified': 'cause-group_Unidentified',\n",
    "    'cause_group_Driver Impairment': 'cause-group_Impairment',\n",
    "    'cause_group_Other': 'cause-group_Other',\n",
    "    'cause_group_Posture': 'cause-group_Posture',\n",
    "    'cause_group_Environmental': 'cause-group_Environmental',\n",
    "    'cause_group_Vehicle': 'cause-group_Vehicle',\n",
    "    }, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f32bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_features_df.to_csv(f\"../{computeddata}/ForModel/all_featuresV2.csv\", index=False)\n",
    "all_features_df = pd.read_csv(f\"../{computeddata}/ForModel/all_featuresV2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84183811",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_resampled_test, y_resampled_test, le =\\\n",
    "     model_preprocess(grid_filter, all_features_df, for_poly=for_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb144220",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_lr = {\n",
    "    'penalty': ['elasticnet'],\n",
    "    'solver': ['saga'],\n",
    "    'l1_ratio': [0.1, 0.5, 0.9],\n",
    "    'C': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "        penalty='elasticnet', solver='saga', l1_ratio=0.5,\n",
    "        class_weight='balanced', max_iter=1000, \n",
    "        random_state=42, \n",
    "        multi_class='multinomial',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "rf = RandomForestClassifier(\n",
    "        n_estimators=300, max_depth=None, min_samples_leaf=1,\n",
    "        class_weight='balanced', n_jobs=-1, random_state=42,\n",
    "    )\n",
    "\n",
    "for name, clf in [('Logistic', lr), ('RandomForest', rf)]:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv, n_jobs=-1, scoring='roc_auc')\n",
    "    print(f'{name} CV ROC AUC: {scores.mean():.3f} ± {scores.std():.3f}')\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5695f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(lr, f'../{computeddata}/ModelPerformance/lr_modelV2.pkl')\n",
    "# joblib.dump(rf, f'../{computeddata}/ModelPerformance/rf_modelV2.pkl')\n",
    "\n",
    "# load model\n",
    "lr = joblib.load(f'../{computeddata}/ModelPerformance/lr_modelV2.pkl')\n",
    "rf = joblib.load(f'../{computeddata}/ModelPerformance/rf_modelV2.pkl')\n",
    "\n",
    "proba_test_lr = lr.predict_proba(X_resampled_test)\n",
    "proba_test_rf = rf.predict_proba(X_resampled_test)\n",
    "y_pred_lr = np.argmax(proba_test_lr, axis=1)\n",
    "y_pred_rf = np.argmax(proba_test_rf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0221e47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_results(proba_test_lr, le.classes_, y_resampled_test)\n",
    "print_results(proba_test_lr, le.classes_, y_resampled_test)\n",
    "print_results(proba_test_rf, le.classes_, y_resampled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4c5164",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_rf, importance_grouped_rf = get_importance(rf, X_train)\n",
    "importance_lr, importance_grouped_lr = get_importance(lr, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aec1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_county_cause_order(importance):\n",
    "    \"\"\"\n",
    "    1. county_彰化縣 x cause_group_Driver-other\n",
    "       -> cause-group_Driver-other x county_彰化縣\n",
    "\n",
    "    2. 單獨的 main effect:\n",
    "       cause_group_Driver-other\n",
    "       -> cause-group_Driver-other\n",
    "    \"\"\"\n",
    "    new_imp = {}\n",
    "\n",
    "    for name, v in importance.items():\n",
    "        # case 1: county_xxx x cause_group_YYY -> cause-group_YYY x county_xxx\n",
    "        if name.startswith(\"county_\") and \" x cause-group_\" in name:\n",
    "            county_part, cg_suffix = name.split(\" x cause-group_\", 1)\n",
    "            # 注意這裡直接用 cause-group_\n",
    "            new_name = f\"cause-group_{cg_suffix} x {county_part}\"\n",
    "            new_imp[new_name] = v\n",
    "\n",
    "        # case 2: 單獨的 cause-group_YYY -> cause-group_YYY\n",
    "        elif name.startswith(\"cause-group_\"):\n",
    "            suffix = name[len(\"cause-group_\"):]  # 取出 Driver-other 那段\n",
    "            new_name = f\"cause-group_{suffix}\"\n",
    "            new_imp[new_name] = v\n",
    "\n",
    "        # 其他都不動\n",
    "        else:\n",
    "            new_imp[name] = v\n",
    "\n",
    "    return new_imp\n",
    "\n",
    "importance_lr = swap_county_cause_order(importance_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23099d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_structured_county(importance):\n",
    "    \"\"\"\n",
    "    importance: dict, 例如 {\"特徵A x county_臺北市\": [coef, OR], ...}\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    for name, v in importance.items():\n",
    "        # 跳過純縣市 dummy，如 \"county_臺北市\"\n",
    "        if name.startswith(\"county_\") and \" x county_\" not in name:\n",
    "            continue\n",
    "        # 只處理有交互字樣的\n",
    "        if \" x county_\" not in name:\n",
    "            continue\n",
    "\n",
    "        base, county_tag = name.split(\" x county_\", 1)  # base 特徵名、county_tag 縣市名（不含前綴）\n",
    "\n",
    "        out.setdefault(county_tag, {})[base] = v[1]\n",
    "\n",
    "    return out\n",
    "\n",
    "structured_all = build_structured_county(importance_lr)\n",
    "structured_all = pd.DataFrame(structured_all)\n",
    "structured_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import col_translation, countycity_dct\n",
    "odds_df = structured_all[['臺北市', '新北市', '臺中市', '高雄市', '臺東縣', '花蓮縣']]\n",
    "\n",
    "odds_df.sort_values(by='臺北市', ascending=False)\n",
    "odds_df = odds_df.rename(columns=countycity_dct, index=col_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b502d789",
   "metadata": {},
   "source": [
    "## Odds Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba47605",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = ['Arial Unicode Ms']\n",
    "\n",
    "df_long = odds_df.stack().reset_index()\n",
    "df_long.columns = ['feature', 'county', 'value']\n",
    "\n",
    "# 用 max|ln(OR)| 當排序依據，抓前 top_k 個以免圖太擁擠\n",
    "# 用 OR 排序會忽略小於 1 的強負向特徵\n",
    "top_k = 30\n",
    "rank = (\n",
    "    df_long.assign(abs_log=lambda d: np.abs(np.log(d['value'])))\n",
    "           .groupby('feature')['abs_log'].max()\n",
    "           .sort_values(ascending=False)\n",
    ")\n",
    "feat_sel = rank.head(top_k).index\n",
    "plot_data = df_long[df_long['feature'].isin(feat_sel)]\n",
    "\n",
    "features = plot_data['feature'].dropna().unique().tolist()\n",
    "features = [f for f in rank.index if f in features]  # 用上面 rank 的順序\n",
    "ypos = {f: i for i, f in enumerate(features)}\n",
    "\n",
    "plt.figure(figsize=(14, max(6, 0.4*len(features))))\n",
    "plt.axvline(1.0, linestyle='--', linewidth=1)\n",
    "plt.axvline(1.5, linestyle='--', linewidth=1)\n",
    "plt.axvline(0.5, linestyle='--', linewidth=1)\n",
    "plt.axvline(2, linestyle='--', linewidth=1)\n",
    "\n",
    "for county, d in plot_data.groupby('county'):\n",
    "    y = [ypos[f] for f in d['feature']]\n",
    "    plt.scatter(d['value'], y, s=20, label=county)\n",
    "\n",
    "plt.yticks(range(len(features)), features)\n",
    "plt.xlabel('Odds Ratio (OR)')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Counties effect per feature (x = OR)')\n",
    "plt.legend(title='County', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39800ee0",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a61d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Neural Network\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "INPUT_DIM = X_resampled_test.shape[1]\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "class BinaryMLP(nn.Module):\n",
    "    def __init__(self, in_dim=INPUT_DIM, num_classes=NUM_CLASSES, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(64, num_classes)  # logits\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d54e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nn, X_val_nn, y_train_nn, y_val_nn = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "X_train_t, y_train_t = to_tensors(X_train, y_train)\n",
    "X_val_t, y_val_t = to_tensors(X_val_nn, y_val_nn)\n",
    "X_test_t, y_test_t = to_tensors(X_resampled_test, y_resampled_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=256, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=512, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=512, shuffle=False)\n",
    "\n",
    "model = BinaryMLP().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val = -np.inf\n",
    "patience = 5\n",
    "wait = 0\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af0f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    val_metrics = eval_loop(model, val_loader, le)\n",
    "    print(f'Epoch {epoch:02d}/{epochs} | loss {train_loss:.4f} | '\n",
    "          f'val_acc {val_metrics[\"acc\"]:.3f} | val_f1 {val_metrics[\"f1\"]:.3f} | val_auc {val_metrics[\"auc\"]:.3f}')\n",
    "\n",
    "    score_for_early = val_metrics[\"auc\"]\n",
    "    if score_for_early > best_val:\n",
    "        best_val = score_for_early\n",
    "        wait = 0\n",
    "        # torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print('Early stopping.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4822f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryMLP().to(device)\n",
    "model.load_state_dict(torch.load(f'../{computeddata}/ModelPerformance/nn_modelV2.pth'))\n",
    "\n",
    "test_metrics = eval_loop(model, test_loader, le)\n",
    "print(test_metrics['report'])\n",
    "\n",
    "# torch.save(model.state_dict(), f'../{computeddata}/ModelPerformance/nn_modelV2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d675f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_model import build_groups_from_prefix, build_groups_with_interactions, build_pair_interaction_groups, PI_ML, PI_NN\n",
    "\n",
    "groups = build_groups_with_interactions(X_test.columns)\n",
    "\n",
    "print('lr')\n",
    "base_lr, perm_lr = PI_ML(lr, X_test, y_test, groups=groups, n_repeats=10)\n",
    "print('rf') \n",
    "base_rf, perm_rf = PI_ML(rf, X_test, y_test, groups=groups, n_repeats=10)\n",
    "print('nn')\n",
    "base_nn, perm_nn = PI_NN(model, X_test, y_test, groups=groups, n_repeats=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perm_lr.to_csv(f'../{computeddata}/Permutation/perm_lrV2.csv')\n",
    "# perm_rf.to_csv(f'../{computeddata}/Permutation/perm_rfV2.csv')\n",
    "# perm_nn.to_csv(f'../{computeddata}/Permutation/perm_nnV2.csv')\n",
    "\n",
    "perm_lr = pd.read_csv(f'../{computeddata}/Permutation/perm_lrV2.csv')\n",
    "perm_rf = pd.read_csv(f'../{computeddata}/Permutation/perm_rfV2.csv')\n",
    "perm_nn = pd.read_csv(f'../{computeddata}/Permutation/perm_nnV2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c5f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([\n",
    "    perm_lr.assign(model='LR'),\n",
    "    # perm_rf.assign(model='RF'),\n",
    "    # perm_nn.assign(model='NN')\n",
    "], ignore_index=True)\n",
    "\n",
    "combined[\"group\"] = combined[\"group\"].map(group_translation)\n",
    "\n",
    "order = (combined.groupby('group')['importance'].mean().sort_values(ascending=True).index.tolist())\n",
    "ypos = np.arange(len(order))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples = [\n",
    "    (perm_lr, 'lr'), \n",
    "    (perm_rf, 'rf'), \n",
    "    (perm_nn, 'nn')\n",
    "    ]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for perm_df_i, name in triples:\n",
    "    perm_df_i[\"group\"] = perm_df_i[\"group\"].map(group_translation)\n",
    "    d = (perm_df_i.set_index('group').reindex(order)) # 用統一群組順序對齊\n",
    "    plt.errorbar(\n",
    "        d['importance'],\n",
    "        (ypos),\n",
    "        xerr=d['std'],\n",
    "        fmt='o',\n",
    "        linewidth=2,\n",
    "        capsize=5,\n",
    "        label=name\n",
    "    )\n",
    "\n",
    "plt.yticks(ypos, order)\n",
    "plt.axvline(0.0, linestyle='--', linewidth=1)\n",
    "plt.xlabel('Permutation importance')\n",
    "plt.ylabel('Group')\n",
    "plt.title('Permutation importance (LR / RF / NN)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435b038f",
   "metadata": {},
   "source": [
    "# Hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5f54c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cols = [col for col in X_resampled_test.columns if (col.startswith('county_') and len(col) == 10)]\n",
    "\n",
    "\n",
    "df_hitrate = X_resampled_test.copy()\n",
    "df_hitrate['y_true'] = y_resampled_test\n",
    "df_hitrate['y_pred'] = y_pred_lr\n",
    "\n",
    "hitrate = {}\n",
    "for col in county_cols:\n",
    "\n",
    "    mask = df_hitrate[df_hitrate[col] != False]\n",
    "    tn, fp, fn, tp = confusion_matrix(\n",
    "        mask['y_true'], mask['y_pred'], labels=[0, 1]\n",
    "    ).ravel()\n",
    "\n",
    "    # calculate precision, recall, accuracy, f1-score\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    hitrate[col] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "hitrate_df = pd.DataFrame.from_dict(hitrate, orient='index', columns=['precision', 'recall', 'accuracy', 'f1']).sort_values('f1', ascending=False)\n",
    "hitrate_df['county'] = hitrate_df.index\n",
    "hitrate_df['county'] = hitrate_df['county'].str.replace('county_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc387668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hitrate_data(resample_X, resample_y, model_y):\n",
    "\n",
    "    county_cols = [col for col in resample_X.columns if (col.startswith('county_') and len(col) == 10)]\n",
    "\n",
    "    df_hitrate = resample_X.copy()\n",
    "    df_hitrate['y_true'] = resample_y\n",
    "    df_hitrate['y_pred'] = model_y\n",
    "\n",
    "    hitrate = {}\n",
    "    for col in county_cols:\n",
    "\n",
    "        mask = df_hitrate[df_hitrate[col] != False]\n",
    "        tn, fp, fn, tp = confusion_matrix(\n",
    "            mask['y_true'], mask['y_pred'], labels=[1, 0] # 這裡0是Hotspot\n",
    "        ).ravel()\n",
    "\n",
    "        # calculate precision, recall, accuracy, f1-score\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        hitrate[col] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'accuracy': accuracy,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "    hitrate_df = pd.DataFrame.from_dict(hitrate, orient='index', columns=['precision', 'recall', 'accuracy', 'f1']).sort_values('f1', ascending=False)\n",
    "    hitrate_df['county'] = hitrate_df.index\n",
    "    hitrate_df['county'] = hitrate_df['county'].str.replace('county_', '')\n",
    "\n",
    "    return hitrate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cede9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_model import metrics_bin\n",
    "# from utils_model import hitrate_data, metrics_bin\n",
    "from config import countycity_dct\n",
    "\n",
    "hitrate_lr = hitrate_data(X_resampled_test, y_resampled_test, y_pred_lr)\n",
    "hitrate_rf = hitrate_data(X_resampled_test, y_resampled_test, y_pred_rf)\n",
    "hitrate_nn = hitrate_data(X_resampled_test, y_resampled_test, test_metrics['pred_y'])\n",
    "\n",
    "hitrate_lr['county'] = hitrate_lr['county'].map(countycity_dct)\n",
    "hitrate_rf['county'] = hitrate_rf['county'].map(countycity_dct)\n",
    "hitrate_nn['county'] = hitrate_nn['county'].map(countycity_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e452498",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'LR': hitrate_lr.copy(),\n",
    "    'RF': hitrate_rf.copy(),\n",
    "    'NN': hitrate_nn.copy(),\n",
    "}\n",
    "order = (results['LR'].sort_values('f1', ascending=False)['county']).tolist()\n",
    "\n",
    "metrics = ['precision', 'recall', 'accuracy', 'f1']\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "for i, met in enumerate(metrics, 1):\n",
    "    ax = plt.subplot(2, 2, i)\n",
    "    pos = np.arange(len(order))\n",
    "    width = 0.25\n",
    "    for j, (name, df) in enumerate(results.items()):\n",
    "        d = df.set_index('county').reindex(order)\n",
    "        ax.bar(pos + (j-1)*width, d[met].values, width=width, label=name)\n",
    "    ax.set_title(met)\n",
    "    ax.set_xticks(pos)\n",
    "    ax.set_xticklabels(order, rotation=45, ha='right')\n",
    "    if i == 2:\n",
    "        ax.legend(loc='upper right')\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cf4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_lr = metrics_bin(y_resampled_test, y_pred_lr)\n",
    "m_rf = metrics_bin(y_resampled_test, y_pred_rf)\n",
    "m_nn = metrics_bin(y_resampled_test, test_metrics['pred_y'])\n",
    "\n",
    "df = pd.DataFrame([m_lr, m_rf, m_nn], index=['LR','RF','NN'])\n",
    "metrics = ['precision', 'recall', 'f1', 'accuracy']\n",
    "df = df[metrics]\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for i, model in enumerate(df.index):\n",
    "    plt.bar(x + (i-1)*width, df.loc[model].values, width=width, label=model)\n",
    "\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Overall Metrics on Resampled Test')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "for i, model in enumerate(df.index):\n",
    "    vals = df.loc[model].values\n",
    "    for xi, v in zip(x + (i-1)*width, vals):\n",
    "        plt.text(xi, v + 0.01, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d28d8c2",
   "metadata": {},
   "source": [
    "Start Analyze importance_lr based on te concept:\n",
    "- human behaviour vs car\n",
    "- car vs road design\n",
    "- road design vs human behavour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba8d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_vs_road(importance):\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    for name, v in importance.items():\n",
    "        if 'x' not in name or 'cause-group' not in name:\n",
    "            continue\n",
    "\n",
    "        base, cause_tag = name.split(\" x \", 1)\n",
    "        out.setdefault(cause_tag, {})[base] = v[1]\n",
    "\n",
    "    return out\n",
    "\n",
    "def car_vs_road(importance):\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    for name, v in importance.items():\n",
    "        if 'x' not in name or '車種' not in name:\n",
    "            continue\n",
    "        \n",
    "        base, cause_tag = name.split(\" x \", 1)\n",
    "        out.setdefault(cause_tag, {})[base] = v[1]\n",
    "\n",
    "    return out\n",
    "\n",
    "def human_vs_car(importance):\n",
    "\n",
    "    out = {}\n",
    "\n",
    "    for name, v in importance.items():\n",
    "        if 'x' not in name or 'cause-group' not in name or '車種' not in name:\n",
    "            continue\n",
    "\n",
    "        base, cause_tag = name.split(\" x \", 1)\n",
    "\n",
    "        out.setdefault(cause_tag, {})[base] = v[1]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interaction(contain_str, structured_cause, countycity_dct=None, col_translation=None, filter_val=None):\n",
    "    odds_df_cause = structured_cause[structured_cause.columns[structured_cause.columns.str.contains(contain_str)]]\n",
    "    odds_df_cause.sort_values(by=odds_df_cause.columns[0], ascending=False)\n",
    "\n",
    "    if countycity_dct and col_translation:\n",
    "        odds_df_cause = odds_df_cause.rename(columns=countycity_dct, index=col_translation)\n",
    "    if filter_val:\n",
    "        odds_df_cause = odds_df_cause[odds_df_cause[odds_df_cause.columns[0]] < filter_val]\n",
    "\n",
    "    plt.rcParams['font.family'] = ['Arial Unicode Ms']\n",
    "    df_long_cause = odds_df_cause.stack().reset_index()\n",
    "    df_long_cause.columns = ['feature', 'cause_group', 'value']\n",
    "    top_k = 100\n",
    "    rank_cause = (\n",
    "        df_long_cause.assign(abs_log=lambda d: np.abs(np.log(d['value'])))\n",
    "            .groupby('feature')['abs_log'].max()\n",
    "            .sort_values(ascending=False)\n",
    "    )\n",
    "    feat_sel_cause = rank_cause.head(top_k).index\n",
    "    plot_data_cause = df_long_cause[df_long_cause['feature'].isin(feat_sel_cause)]\n",
    "    features_cause = plot_data_cause['feature'].dropna().unique().tolist()\n",
    "    features_cause = [f for f in rank_cause.index if f in features_cause]\n",
    "    ypos_cause = {f: i for i, f in enumerate(features_cause)}  \n",
    "    plt.figure(figsize=(14, max(6, 0.4*len(features_cause))))\n",
    "    plt.axvline(1.0, linestyle='--', linewidth=1)\n",
    "    plt.axvline(1.5, linestyle='--', linewidth=1)\n",
    "    plt.axvline(0.5, linestyle='--', linewidth=1)\n",
    "    plt.axvline(2, linestyle='--', linewidth=1)\n",
    "    for cause_group, d in plot_data_cause.groupby('cause_group'):\n",
    "        y = [ypos_cause[f] for f in d['feature']]\n",
    "        plt.scatter(d['value'], y, s=20, label=cause_group)\n",
    "    plt.yticks(range(len(features_cause)), features_cause)\n",
    "    plt.xlabel('Odds Ratio (OR)')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Effect per feature (x = OR)')\n",
    "    plt.legend(title=contain_str, bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c653dcc5",
   "metadata": {},
   "source": [
    "## Car vs Road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46debd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_cause = car_vs_road(importance_lr)\n",
    "structured_cause = pd.DataFrame(structured_cause)\n",
    "structured_cause = structured_cause[~structured_cause.index.str.contains('車輛撞擊')]\n",
    "\n",
    "plot_interaction('車種', structured_cause, filter_val=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb50762",
   "metadata": {},
   "source": [
    "## Human vs Road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aedf9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_cause = human_vs_road(importance_lr)\n",
    "structured_cause = pd.DataFrame(structured_cause)\n",
    "structured_cause = structured_cause[~structured_cause.index.str.contains('車種')]\n",
    "structured_cause = structured_cause[~structured_cause.index.str.contains('車輛')]\n",
    "\n",
    "plot_interaction('cause-group', structured_cause, filter_val=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cdaffa",
   "metadata": {},
   "source": [
    "## Human vs Car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_cause = human_vs_car(importance_lr)\n",
    "structured_cause = pd.DataFrame(structured_cause)\n",
    "\n",
    "plot_interaction('cause-group', structured_cause, filter_val=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ST-RTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
