{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce50f96e",
   "metadata": {},
   "source": [
    "ComputedDataV3: Include facilities (youbike, parkinglot, mrt) <br/>\n",
    "ComputedDataV3: Remove facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20870616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "analyze_path = os.path.join(parent_dir, \"utils\")\n",
    "\n",
    "os.chdir(analyze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = ['Arial Unicode Ms']\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, average_precision_score\n",
    "import torch\n",
    "from utils_nn import BinaryMLP, train_neural_network, predict_nn\n",
    "from utils_model import model_preprocess, print_results, get_importance\n",
    "\n",
    "from utils import read_data, read_taiwan_specific\n",
    "from config import cause_mapping, countycity_dct\n",
    "from config_new import for_poly, group_translation\n",
    "\n",
    "version = \"V1\"\n",
    "computeddata = 'ComputedDataV3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from utils import get_grid, read_data, read_taiwan_specific\n",
    "from utils_macro import GetisOrdGiAnalysis\n",
    "\n",
    "version = 'V1'\n",
    "ComputedDataVersion = 'V2'\n",
    "\n",
    "combined_data = read_data()\n",
    "taiwan, _grid_filter = read_taiwan_specific(read_grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba580a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "import geopandas as gpd\n",
    "\n",
    "hex_grid = pd.read_csv(f'../ComputedData{ComputedDataVersion}/Grid/hex_grid{version}.csv')\n",
    "hex_grid['geometry'] = hex_grid['geometry'].apply(wkt.loads)\n",
    "hex_grid = gpd.GeoDataFrame(hex_grid, geometry='geometry', crs=\"EPSG:3826\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f158dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start GI')\n",
    "go = GetisOrdGiAnalysis(hex_grid, taiwan)\n",
    "go.calculate_gi(best_distance=6, adjacency='queen')\n",
    "grid_gi = go.grid\n",
    "grid_gi.to_csv(f'../ComputedData{ComputedDataVersion}/Grid/grid_giV2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d75b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "go.plot_gi_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b971db",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = read_data()\n",
    "# taiwan, grid_filter = read_taiwan_specific(read_grid=True)\n",
    "\n",
    "def map_cause(cause):\n",
    "    for category, causes in cause_mapping.items():\n",
    "        if cause in causes:\n",
    "            return category\n",
    "    return \"Unknown\"\n",
    "\n",
    "combined_data[\"cause_group\"] = combined_data[\"肇因研判子類別名稱-主要\"].apply(map_cause)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeec8b9f",
   "metadata": {},
   "source": [
    "本資料夾主要完成三個模型：\n",
    "- 模型1: (一維)人車路\n",
    "- 模型2: (二維)人車沒有路 (加法、乘法交互) (一維有路但沒交互&一維沒有路且沒交互)\n",
    "- 模型3: (三維)人車路 (加法、乘法交互)\n",
    "\n",
    "| 需要去掉市區道路(道路類別-第1當事者-名稱)、縣市(county)這種欄位，專注在道路設計（路）、肇因（人）、車種（車）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7521889",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_df = pd.read_csv(f\"../ComputedDataV2/ForModel/all_featuresV2.csv\")\n",
    "all_features_df = all_features_df[all_features_df.columns[~all_features_df.columns.str.contains('道路類別-第1當事者-名稱|county|路面狀況|道路障礙|車輛撞擊部位大類別名稱|original-speed')]]\n",
    "# all_features_df = all_features_df[all_features_df.columns[~all_features_df.columns.str.contains('道路類別-第1當事者-名稱|county|youbike|mrt|parkinglot')]] # for ComputedDataV4\n",
    "print(all_features_df.shape)\n",
    "\n",
    "road_features = [\n",
    "    '車道劃分設施-分道設施-快車道或一般車道間名稱',\n",
    "    '車道劃分設施-分道設施-快慢車道間名稱',\n",
    "    '車道劃分設施-分道設施-路面邊線名稱',\n",
    "    '車道劃分設施-分向設施大類別名稱',\n",
    "    '事故類型及型態大類別名稱',\n",
    "    '道路型態大類別名稱',\n",
    "    '號誌-號誌種類名稱',\n",
    "    '速限-第1當事者',\n",
    "    'youbike_100m_count',\n",
    "    'mrt_100m_count',\n",
    "    'parkinglot_100m_count',\n",
    "]\n",
    "car_features = [\n",
    "    '當事者區分-類別-大類別名稱-車種',\n",
    "]\n",
    "person_features = [\n",
    "    'cause-group',\n",
    "]\n",
    "\n",
    "# This is to check if all features are in the dataframe columns\n",
    "for feature in road_features + car_features + person_features:\n",
    "    match_found = any(feature in col for col in all_features_df.columns)\n",
    "    print(f\"Checking feature: {feature}, Found: {match_found}\")\n",
    "    \n",
    "    assert match_found, f\"Feature {feature} not found in dataframe columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb078fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_keywords = road_features + car_features + person_features\n",
    "matched_cols = set()\n",
    "for col in all_features_df.columns:\n",
    "    for kw in defined_keywords:\n",
    "        if kw in col:\n",
    "            matched_cols.add(col)\n",
    "            break\n",
    "\n",
    "all_cols = set(all_features_df.columns)\n",
    "extra_cols = all_cols - matched_cols\n",
    "\n",
    "print(f\"Dataframe 總欄位數: {len(all_cols)}\")\n",
    "print(f\"已被關鍵字涵蓋的欄位數: {len(matched_cols)}\")\n",
    "print(f\"額外欄位數: {len(extra_cols)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if len(extra_cols) > 0:\n",
    "    print(\"額外欄位:\")\n",
    "    for col in sorted(list(extra_cols)):\n",
    "        print(col)\n",
    "else:\n",
    "    print(\"所有欄位都已在列表\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9151fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_model = {\n",
    "    'model_1': {\n",
    "        'grid_filter': None, # None is set to not include county data\n",
    "        'dim': '1way',\n",
    "        'base_road': road_features,\n",
    "        'base_vehicle': car_features,\n",
    "        'base_person': person_features,\n",
    "        'interaction_type': None,\n",
    "        'model': [],\n",
    "    },\n",
    "    'model_2_multiply_cp': {\n",
    "        'grid_filter': None,\n",
    "        'dim': '2way',\n",
    "        'base_road': None,\n",
    "        'base_vehicle': car_features,\n",
    "        'base_person': person_features,\n",
    "        'interaction_type': 'multiply',\n",
    "        'model': [],\n",
    "    },\n",
    "    'model_2_add_cp': {\n",
    "        'grid_filter': None,\n",
    "        'dim': '2way',\n",
    "        'base_road': None,\n",
    "        'base_vehicle': car_features,\n",
    "        'base_person': person_features,\n",
    "        'interaction_type': 'add',\n",
    "        'model': [],\n",
    "    },\n",
    "    'model_3_multiply': {\n",
    "        'grid_filter': None,\n",
    "        'dim': 'mixed',\n",
    "        'base_road': road_features,\n",
    "        'base_vehicle': car_features,\n",
    "        'base_person': person_features,\n",
    "        'interaction_type': 'multiply',\n",
    "        'model': [],\n",
    "    },\n",
    "    'model_3_add': {\n",
    "        'grid_filter': None,\n",
    "        'dim': 'mixed',\n",
    "        'base_road': road_features,\n",
    "        'base_vehicle': car_features,\n",
    "        'base_person': person_features,\n",
    "        'interaction_type': 'add',\n",
    "        'model': [],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1c80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(\n",
    "        X_train, y_train, X_resampled_test, y_resampled_test, le, model_name, set_grid_search=True\n",
    "        ):\n",
    "    param_grid_lr = {\n",
    "        'C': [0.1, 1, 10, 100], \n",
    "        'l1_ratio': [0.1, 0.5, 0.9]\n",
    "    }\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [100, 300],\n",
    "        'max_depth': [10, 20],\n",
    "        'min_samples_leaf': [1, 4]\n",
    "    }\n",
    "\n",
    "    lr_refined = LogisticRegression(\n",
    "        penalty='elasticnet', solver='saga', \n",
    "        class_weight='balanced', max_iter=1000, \n",
    "        random_state=42, n_jobs=-1\n",
    "        )\n",
    "    rf_refined = RandomForestClassifier(\n",
    "        class_weight='balanced', random_state=42, n_jobs=-1\n",
    "        )\n",
    "\n",
    "    if set_grid_search:\n",
    "\n",
    "        grid_lr = GridSearchCV(lr_refined, param_grid_lr, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "        grid_rf = GridSearchCV(rf_refined, param_grid_rf, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "        grid_lr.fit(X_train, y_train)\n",
    "        lr = grid_lr.best_estimator_\n",
    "        print(f\"LR best params: {lr}\")\n",
    "        grid_rf.fit(X_train, y_train)\n",
    "        rf = grid_rf.best_estimator_\n",
    "        print(f\"RF best params: {rf}\")\n",
    "\n",
    "    else:\n",
    "        \n",
    "        lr = LogisticRegression(\n",
    "            penalty='elasticnet', solver='saga', l1_ratio=0.5,\n",
    "            class_weight='balanced', max_iter=1000, \n",
    "            random_state=42, \n",
    "            multi_class='multinomial',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf = RandomForestClassifier(\n",
    "            n_estimators=300, max_depth=None, min_samples_leaf=1,\n",
    "            class_weight='balanced', n_jobs=-1, random_state=42,\n",
    "        )\n",
    "        print('logistic regression training')\n",
    "        lr.fit(X_train, y_train)\n",
    "        print('random forest training')\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "    # nn 沒有grid search\n",
    "    nn_final = train_neural_network(\n",
    "        X_train, y_train, le=le,\n",
    "        input_dim=X_train.shape[1]\n",
    "    )\n",
    "\n",
    "    joblib.dump(lr, f'../{computeddata}/ModelPerformance/lr_model{version}_{model_name}.pkl')\n",
    "    joblib.dump(rf, f'../{computeddata}/ModelPerformance/rf_model{version}_{model_name}.pkl')\n",
    "    torch.save(nn_final.state_dict(), f'../{computeddata}/ModelPerformance/nn_model{version}_{model_name}.pt')\n",
    "    \n",
    "    proba_test_lr = lr.predict_proba(X_resampled_test)\n",
    "    proba_test_rf = rf.predict_proba(X_resampled_test)\n",
    "    proba_test_nn = predict_nn(nn_final, X_resampled_test)\n",
    "\n",
    "    print_results(proba_test_lr, le.classes_, y_resampled_test)\n",
    "    print_results(proba_test_rf, le.classes_, y_resampled_test)\n",
    "    print_results(proba_test_nn, le.classes_, y_resampled_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83028743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型區分兩段一維、二維一組，三維一組，原因是三維資料特徵數量太高需要額外篩選，而一、二維可直接做grid search\n",
    "for model_name, config in dct_model.items():\n",
    "    set_grid_search = True\n",
    "    print(f\"training model: {model_name}\")\n",
    "    X_train, X_test, y_train, y_test, X_resampled_test, y_resampled_test, le = model_preprocess(\n",
    "        all_features_df, \n",
    "        grid_filter=config['grid_filter'],\n",
    "        dim=config['dim'],\n",
    "        base_road=config['base_road'],\n",
    "        base_vehicle=config['base_vehicle'],\n",
    "        base_person=config['base_person'],\n",
    "        interaction_type=config['interaction_type'],\n",
    "    )\n",
    "    if model_name == 'model_3_multiply' or model_name == 'model_3_add':\n",
    "        set_grid_search = False\n",
    "        print(\"Skip grid search for model_3 variants\")\n",
    "    else:\n",
    "        set_grid_search = True\n",
    "\n",
    "    model_training(\n",
    "        X_train, y_train, X_resampled_test, y_resampled_test, le, model_name, set_grid_search=set_grid_search\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d20e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extracted_features(lr, rf, X_resampled_test):\n",
    "    feature_order = lr.feature_names_in_\n",
    "    X_sorted = X_resampled_test.reindex(columns=feature_order, fill_value=0)\n",
    "\n",
    "    importance_lr_dict, _ = get_importance(lr, X_sorted)\n",
    "    df_lr_ranked = pd.DataFrame.from_dict(importance_lr_dict, orient='index', columns=['score', 'exp_score'])\n",
    "    df_lr_ranked['abs_score'] = df_lr_ranked['score'].abs()\n",
    "    importance_rf_dict, _ = get_importance(rf, X_sorted)\n",
    "    df_rf_ranked = pd.DataFrame.from_dict(importance_rf_dict, orient='index', columns=['score', 'exp_score'])\n",
    "    df_rf_ranked['abs_score'] = df_rf_ranked['score'].abs()\n",
    "\n",
    "    def get_dimension(feature_name):\n",
    "        x_count = feature_name.count(' x ')\n",
    "        if x_count == 0: return 1\n",
    "        if x_count == 1: return 2 \n",
    "        if x_count == 2: return 3\n",
    "        return 0\n",
    "\n",
    "    df_lr_ranked['dimension'] = df_lr_ranked.index.map(get_dimension)\n",
    "    df_rf_ranked['dimension'] = df_rf_ranked.index.map(get_dimension)\n",
    "\n",
    "    lr_dim1 = df_lr_ranked[df_lr_ranked['dimension'] == 1].index.tolist()\n",
    "    lr_dim2 = df_lr_ranked[df_lr_ranked['dimension'] == 2].sort_values('abs_score', ascending=False).head(20).index.tolist()\n",
    "    lr_dim3 = df_lr_ranked[df_lr_ranked['dimension'] == 3].sort_values('abs_score', ascending=False).head(20).index.tolist()\n",
    "    rf_dim1 = df_rf_ranked[df_rf_ranked['dimension'] == 1].index.tolist()\n",
    "    rf_dim2 = df_rf_ranked[df_rf_ranked['dimension'] == 2].sort_values('abs_score', ascending=False).head(20).index.tolist()\n",
    "    rf_dim3 = df_rf_ranked[df_rf_ranked['dimension'] == 3].sort_values('abs_score', ascending=False).head(20).index.tolist()\n",
    "\n",
    "    final_dim1 = list(set(lr_dim1 + rf_dim1))\n",
    "    final_dim2 = list(set(lr_dim2 + rf_dim2))\n",
    "    final_dim3 = list(set(lr_dim3 + rf_dim3))\n",
    "    final_feature_list = final_dim1 + final_dim2 + final_dim3\n",
    "\n",
    "    print(f\"Original feature: {len(final_dim1)}\")\n",
    "    print(f\"Two-way feature: {len(final_dim2)} (LR and RF union)\")\n",
    "    print(f\"Three-way feature: {len(final_dim3)} (LR and RF union)\")\n",
    "    print(f\"Final model feature total: {len(final_feature_list)}\")\n",
    "\n",
    "    print(\"top 3 way\")\n",
    "    print(df_lr_ranked[df_lr_ranked['dimension'] == 3].sort_values('abs_score', ascending=False).head(5).index.tolist())\n",
    "\n",
    "    return final_feature_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafca3b5",
   "metadata": {},
   "source": [
    "## This is for extract feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47c4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 這裡寫法還是add, multiply各別跑\n",
    "tp = 'add'\n",
    "lr = joblib.load(f'../{computeddata}/ModelPerformance/lr_modelV1_model_3_{tp}.pkl')\n",
    "rf = joblib.load(f'../{computeddata}/ModelPerformance/rf_modelV1_model_3_{tp}.pkl')\n",
    "\n",
    "X_train, X_test, y_train, y_test, X_resampled_test, y_resampled_test, le = model_preprocess(\n",
    "         all_features_df, grid_filter=None, dim='mixed',base_road=road_features,\n",
    "         base_vehicle=car_features, base_person=person_features,interaction_type=tp)\n",
    "\n",
    "final_feature_list = get_extracted_features(lr, rf, X_resampled_test)\n",
    "\n",
    "X_train_refined = X_train[final_feature_list]\n",
    "\n",
    "X_train_sorted = X_train_refined.reindex(columns=final_feature_list, fill_value=0)\n",
    "X_train_resampled_reordered = X_train_sorted.reindex(columns=final_feature_list, fill_value=0)\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1, 10, 100], \n",
    "    'l1_ratio': [0.1, 0.5, 0.9]\n",
    "}\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [10, 20],\n",
    "    'min_samples_leaf': [1, 4]\n",
    "}\n",
    "\n",
    "# lr_refined = LogisticRegression(\n",
    "#     penalty='elasticnet', solver='saga', \n",
    "#     class_weight='balanced', max_iter=1000, \n",
    "#     random_state=42, n_jobs=-1\n",
    "# )\n",
    "# rf_refined = RandomForestClassifier(class_weight='balanced', random_state=42, n_jobs=-1)\n",
    "\n",
    "# grid_lr = GridSearchCV(lr_refined, param_grid_lr, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "# grid_rf = GridSearchCV(rf_refined, param_grid_rf, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "\n",
    "# grid_lr.fit(X_train_refined, y_train)\n",
    "# grid_rf.fit(X_train_refined, y_train)\n",
    "print(X_train_resampled_reordered.shape)\n",
    "nn_final = train_neural_network(\n",
    "    X_train_resampled_reordered, y_train, le=le,\n",
    "    input_dim=X_train_resampled_reordered.shape[1]\n",
    ")\n",
    "\n",
    "# best_lr_model = grid_lr.best_estimator_\n",
    "# best_rf_model = grid_rf.best_estimator_\n",
    "# print(f\"LR best params: {grid_lr.best_params_}\")\n",
    "# print(f\"RF best params: {grid_rf.best_params_}\")\n",
    "\n",
    "# joblib.dump(best_lr_model, f'../{computeddata}/ModelPerformance/lr_model{version}_model_3_{tp}_extracted.pkl')\n",
    "# joblib.dump(best_rf_model, f'../{computeddata}/ModelPerformance/rf_model{version}_model_3_{tp}_extracted.pkl')\n",
    "torch.save(nn_final.state_dict(), f'../{computeddata}/ModelPerformance/nn_model{version}_model_3_{tp}_extracted.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ffb03",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2322325",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "file_suffix_map = {\n",
    "    'model_1': 'model_1',\n",
    "    'model_2_add_cp': 'model_2_add_cp',\n",
    "    'model_2_multiply_cp': 'model_2_multiply_cp',\n",
    "    'model_3_add': 'model_3_add_extracted',\n",
    "    'model_3_multiply': 'model_3_multiply_extracted',\n",
    "}\n",
    "\n",
    "for model_name, config in dct_model.items():\n",
    "    print(f\"\\n=== Processing: {model_name} ===\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, X_resampled_test, y_resampled_test, le = model_preprocess(\n",
    "        all_features_df, \n",
    "        grid_filter=config['grid_filter'],\n",
    "        dim=config['dim'],\n",
    "        base_road=config['base_road'],\n",
    "        base_vehicle=config['base_vehicle'],\n",
    "        base_person=config['base_person'],\n",
    "        interaction_type=config['interaction_type'],\n",
    "    )\n",
    "\n",
    "    suffix = file_suffix_map.get(model_name, model_name) \n",
    "    base_path = f'../{computeddata}/ModelPerformance'\n",
    "\n",
    "    if model_name in ['model_3_add', 'model_3_multiply']:\n",
    "        lr = joblib.load(f'../{computeddata}/ModelPerformance/lr_modelV1_{model_name}.pkl')\n",
    "        rf = joblib.load(f'../{computeddata}/ModelPerformance/rf_modelV1_{model_name}.pkl')\n",
    "        final_feature_list = get_extracted_features(lr, rf, X_resampled_test)    \n",
    "        X_train_refined = X_train[final_feature_list]\n",
    "    # X_test_refined = X_resampled_test[final_feature_list]\n",
    "    try:\n",
    "\n",
    "        lr_path = f'{base_path}/lr_modelV1_{suffix}.pkl'\n",
    "        lr_model = joblib.load(lr_path)\n",
    "        print(f\"  -> Loaded LR: {lr_path}\")\n",
    "\n",
    "        rf_path = f'{base_path}/rf_modelV1_{suffix}.pkl'\n",
    "        rf_model = joblib.load(rf_path)\n",
    "        print(f\"  -> Loaded RF: {rf_path}\")\n",
    "\n",
    "        if model_name in ['model_3_add', 'model_3_multiply']:\n",
    "            input_dim = X_train_refined.shape[1]\n",
    "            print(f\"  -> Using refined input dim: {input_dim}\")\n",
    "        else:\n",
    "            input_dim = X_train.shape[1]\n",
    "            print(f\"  -> Using input dim: {input_dim}\")\n",
    "        nn_model = BinaryMLP(in_dim=input_dim).to(device)\n",
    "        nn_path = f'{base_path}/nn_modelV1_{suffix}.pt'\n",
    "        nn_model.load_state_dict(torch.load(nn_path, map_location=device))\n",
    "        print(f\"  -> Loaded NN: {nn_path} (dim={input_dim})\")\n",
    "\n",
    "        config['model'] = [lr_model, rf_model, nn_model]\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\" [ERROR] File not found: {e}\")\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcbdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re fit for only important features\n",
    "def print_results(proba_test, classes, y_resampled_test):\n",
    "    \"\"\"\n",
    "    proba_test: 預測的概率\n",
    "    classes: 類別名稱\n",
    "    y_resampled_test: 重抽樣後的測試標籤\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    y_pred = np.argmax(proba_test, axis=1)\n",
    "\n",
    "    print(\"Confusion Matrix\")\n",
    "    conf_matrix = confusion_matrix(y_resampled_test, y_pred, labels=range(len(classes)))\n",
    "    print(conf_matrix)\n",
    "\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(\n",
    "        y_resampled_test, y_pred, target_names=classes, digits=3\n",
    "    ))\n",
    "\n",
    "    if proba_test.shape[1] == 2:\n",
    "        # 二元分類\n",
    "        roc_auc = roc_auc_score(y_resampled_test, proba_test[:, 1])\n",
    "        print(f'ROC AUC: {roc_auc:.3f}')\n",
    "        y_test_bin = label_binarize(y_resampled_test, classes=range(len(classes)))\n",
    "        pr_auc_macro  = average_precision_score(y_test_bin, proba_test[:, 1], average='macro')\n",
    "        pr_auc_weight = average_precision_score(y_test_bin, proba_test[:, 1], average='weighted')\n",
    "        print(f'PR  AUC macro: {pr_auc_macro:.3f}')\n",
    "        print(f'PR  AUC wighted: {pr_auc_weight:.3f}')\n",
    "\n",
    "    return conf_matrix\n",
    "\n",
    "all_performance = {}\n",
    "all_importance = {}\n",
    "for model_name, config in dct_model.items():\n",
    "\n",
    "    print(f\"training model: {model_name}\")\n",
    "    X_train, X_test, y_train, y_test, X_resampled_test, y_resampled_test, le = model_preprocess(\n",
    "        all_features_df, \n",
    "        grid_filter=config['grid_filter'],\n",
    "        dim=config['dim'],\n",
    "        base_road=config['base_road'],\n",
    "        base_vehicle=config['base_vehicle'],\n",
    "        base_person=config['base_person'],\n",
    "        interaction_type=config['interaction_type'],\n",
    "    )\n",
    "    # don't need two reorder because they are using the same columns in training\n",
    "    feature_order_lr = config['model'][0].feature_names_in_\n",
    "\n",
    "    if model_name == 'model_3_add' or model_name == 'model_3_multiply':\n",
    "        final_feature_list = get_extracted_features(config['model'][0], config['model'][1], X_resampled_test)\n",
    "        X_resampled_test = X_resampled_test[final_feature_list]\n",
    "\n",
    "    X_sorted = X_resampled_test.reindex(columns=feature_order_lr, fill_value=0)\n",
    "    X_resampled_reordered = X_sorted.reindex(columns=feature_order_lr, fill_value=0)\n",
    "\n",
    "    proba_test_lr = config['model'][0].predict_proba(X_resampled_reordered)\n",
    "    proba_test_rf = config['model'][1].predict_proba(X_resampled_reordered)\n",
    "    proba_test_nn = predict_nn(config['model'][2], X_resampled_reordered)\n",
    "    \n",
    "    # print('model performance for lr')\n",
    "    # print_results(proba_test_lr, le.classes_, y_resampled_test)\n",
    "    # print('model performance for rf')\n",
    "    # print_results(proba_test_rf, le.classes_, y_resampled_test)\n",
    "\n",
    "    all_performance[model_name] = {\n",
    "        'lr': print_results(proba_test_lr, le.classes_, y_resampled_test),\n",
    "        'rf': print_results(proba_test_rf, le.classes_, y_resampled_test),\n",
    "        'nn': print_results(proba_test_nn, le.classes_, y_resampled_test),\n",
    "    }\n",
    "\n",
    "    importance_lr, importance_grouped_lr = get_importance(config['model'][0], X_resampled_reordered)\n",
    "    importance_rf, importance_grouped_rf = get_importance(config['model'][1], X_resampled_reordered)\n",
    "\n",
    "    all_importance[model_name] = {\n",
    "        'lr': [importance_lr, importance_grouped_lr],\n",
    "        'rf': [importance_rf, importance_grouped_rf],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b846380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "records = []\n",
    "\n",
    "for model_name, algos in all_performance.items():\n",
    "    for algo_name, cm in algos.items():\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        records.append({\n",
    "            'Model': model_name,\n",
    "            'Algorithm': algo_name,\n",
    "            'Accuracy': accuracy,\n",
    "            'F1-Score': f1,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "sns.barplot(data=df, x='Model', y='Accuracy', hue='Algorithm', ax=axes[0, 0], palette='viridis')\n",
    "axes[0, 0].set_title('Model Accuracy Comparison')\n",
    "axes[0, 0].set_ylim(0, 1.1)\n",
    "axes[0, 0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "for container in axes[0, 0].containers:\n",
    "    axes[0, 0].bar_label(container, fmt='%.3f')\n",
    "\n",
    "sns.barplot(data=df, x='Model', y='F1-Score', hue='Algorithm', ax=axes[0, 1], palette='viridis')\n",
    "axes[0, 1].set_title('Model F1-Score Comparison')\n",
    "axes[0, 1].set_ylim(0, 1.1)\n",
    "axes[0, 1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "for container in axes[0, 1].containers:\n",
    "    axes[0, 1].bar_label(container, fmt='%.3f')\n",
    "\n",
    "sns.barplot(data=df, x='Model', y='Precision', hue='Algorithm', ax=axes[1, 0], palette='viridis')\n",
    "axes[1, 0].set_title('Model Precision Comparison')\n",
    "axes[1, 0].set_ylim(0, 1.1)\n",
    "axes[1, 0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "for container in axes[1, 0].containers:\n",
    "    axes[1, 0].bar_label(container, fmt='%.3f')\n",
    "\n",
    "sns.barplot(data=df, x='Model', y='Recall', hue='Algorithm', ax=axes[1, 1], palette='viridis')\n",
    "axes[1, 1].set_title('Model Recall Comparison')\n",
    "axes[1, 1].set_ylim(0, 1.1)\n",
    "axes[1, 1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "for container in axes[1, 1].containers:\n",
    "    axes[1, 1].bar_label(container, fmt='%.3f')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007a45c",
   "metadata": {},
   "source": [
    "## This is only for model with more than 2 interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5291bd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "name = 'model_3_multiply'\n",
    "model_name = 'lr'\n",
    "data = all_importance[name][model_name][0]\n",
    "df_importance = pd.DataFrame.from_dict(data, orient='index', columns=['Coefficient', 'Odds_Ratio'])\n",
    "df_importance = df_importance.sort_values(by='Coefficient', ascending=False)\n",
    "df_importance = df_importance[df_importance.index != '車道劃分設施-分道設施-路面邊線名稱_無']\n",
    "\n",
    "df_importance_plot = df_importance.head(10).copy()\n",
    "df_importance_plot = df_importance_plot.sort_values(by='Coefficient', ascending=True)\n",
    "def get_dimension(feature_name):\n",
    "    return feature_name.count(' x ') + 1\n",
    "\n",
    "df_importance_plot['Dimension'] = df_importance_plot.index.map(get_dimension)\n",
    "\n",
    "dim_colors = {1: '#5DADE2', 2: '#F5B041', 3: '#EC7063'} \n",
    "bar_colors = df_importance_plot['Dimension'].map(dim_colors)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "ylabels = [label.replace(' x ', '\\n + ') for label in df_importance_plot.index]\n",
    "\n",
    "bars = ax.barh(\n",
    "    range(len(df_importance_plot)), \n",
    "    df_importance_plot['Coefficient'], \n",
    "    color=bar_colors\n",
    ")\n",
    "\n",
    "ax.set_yticks(range(len(df_importance_plot)))\n",
    "ax.set_yticklabels(ylabels, fontsize=10)\n",
    "plt.subplots_adjust(left=0.25, right=0.95, top=0.9, bottom=0.1)\n",
    "\n",
    "ax.set_xlabel('Coefficient / Importance')\n",
    "ax.set_title(f'Top 10 Feature Coefficients {name} {model_name}', fontsize=14)\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "\n",
    "legend_handles = [\n",
    "    mpatches.Patch(color=dim_colors[1], label='1-Way'),\n",
    "    mpatches.Patch(color=dim_colors[2], label='2-Way Interaction'),\n",
    "    mpatches.Patch(color=dim_colors[3], label='3-Way Interaction')\n",
    "]\n",
    "ax.legend(handles=legend_handles, loc='lower right', title=\"Feature Dimension\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f0b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = all_importance['model_3_multiply']['lr'][0]\n",
    "important_groups = []\n",
    "for k, v in importance.items():\n",
    "    if v[1] > 1:\n",
    "        important_groups.append((k, v))\n",
    "\n",
    "df = pd.DataFrame(important_groups, columns=['feature_name', 'scores'])\n",
    "df['importance'] = df['scores'].apply(lambda x: x[0])\n",
    "df['odds_ratio'] = df['scores'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63859b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_interaction(row):\n",
    "    parts = row.split(' x ')\n",
    "    road, vehicle, person = \"無\", \"無\", \"無\"\n",
    "    \n",
    "    for p in parts:\n",
    "        p_clean = p.replace('_mean', '')\n",
    "        \n",
    "        if 'county' in p_clean:\n",
    "            # county_臺北市 -> 臺北市\n",
    "            road = p_clean.split('_')[-1]\n",
    "            \n",
    "        # 只改POI\n",
    "        elif 'count' in p_clean:\n",
    "            if 'parkinglot' in p_clean: road = '停車場密度'\n",
    "            elif 'youbike' in p_clean: road = 'YouBike密度'\n",
    "            elif 'mrt' in p_clean: road = '捷運密度'\n",
    "            else: road = p_clean\n",
    "\n",
    "        elif '速限' in p_clean:\n",
    "            road = '速限'\n",
    "\n",
    "        # 道路\n",
    "        # elif any(x in p_clean for x in ['道路', '號誌', '路面', '障礙', '事故類型']):\n",
    "        #     if '_' in p_clean:\n",
    "        #         road = p_clean.split('_')[-1] # 取底線後的值\n",
    "        #     else:\n",
    "        #         road = p_clean\n",
    "\n",
    "        elif any(x in p_clean for x in ['道路', '號誌', '路面', '障礙', '事故類型', '車道', '設施']):\n",
    "            val = p_clean.split('_')[-1] if '_' in p_clean else p_clean\n",
    "            \n",
    "            # \"有\" 或 \"無\" 有包含多種分向設施\n",
    "            if val in ['有', '無']:\n",
    "                # 找出前綴 (例如 \"路面邊線名稱\")\n",
    "                prefix = p_clean.split('_')[0]\n",
    "                # 簡化前綴 (只取最後幾個字，例如 \"路面邊線\")\n",
    "                simple_prefix = prefix\n",
    "                if '名稱' in prefix: simple_prefix = prefix.replace('名稱', '').replace('大類別', '')\n",
    "                if '-' in simple_prefix: simple_prefix = simple_prefix.split('-')[-1]\n",
    "                \n",
    "                road = f\"{val}{simple_prefix}\"\n",
    "            else:\n",
    "                road = val\n",
    "\n",
    "        # 車\n",
    "        elif any(x in p_clean for x in ['車種', '撞擊', '當事者']):\n",
    "            if '_' in p_clean:\n",
    "                vehicle = p_clean.split('_')[-1]\n",
    "            else:\n",
    "                vehicle = p_clean\n",
    "\n",
    "        # 人\n",
    "        elif 'cause' in p_clean:\n",
    "             if '_' in p_clean:\n",
    "                person = p_clean.split('_')[-1]\n",
    "             else:\n",
    "                person = p_clean\n",
    "                \n",
    "    dim = len(parts)\n",
    "    return pd.Series([road, vehicle, person, dim])\n",
    "\n",
    "df[['Road', 'Vehicle', 'Person', 'Dimension']] = df['feature_name'].apply(parse_interaction)\n",
    "# df = df.replace(\"機車\", \"機車與自行車\")\n",
    "# df = df.replace(\"慢車\", \"機車與自行車\")\n",
    "# '機車', '慢車', \n",
    "df = df[~df['Vehicle'].isin(['汽車', '機車與自行車'])]\n",
    "df = df[~df['Road'].str[2].isin(['市', '縣'])]\n",
    "df = df[~((df['Vehicle'] == '人') & (df['Road'] == '車與車'))] # 移除異常\n",
    "df = df[(df['Road'] != '車與車') & (df['Road'] != '人與車')] # 雖然子類別也是道路型態，但會誤導\n",
    "# df = df.head(30)\n",
    "\n",
    "global_min = 0  \n",
    "global_max = df['importance'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c2deb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df_3 = df[df['Dimension'] == 3].head(30)\n",
    "df_plot = df_3.copy()\n",
    "\n",
    "df_plot = df_plot.sort_values('importance', ascending=False)\n",
    "fig_parcats = px.parallel_categories(\n",
    "    df_plot, \n",
    "    dimensions=['Road', 'Vehicle', 'Person'],\n",
    "    color=\"importance\",\n",
    "    color_continuous_scale=px.colors.sequential.Inferno,\n",
    "    labels={'Road':'道路設計', 'Vehicle':'車種', 'Person':'肇因'},\n",
    "    range_color=[global_min, global_max]\n",
    ")\n",
    "\n",
    "fig_parcats.update_layout(\n",
    "    title=\"三維交互作用圖\",\n",
    "    height=800,\n",
    "    width=1500\n",
    ")\n",
    "\n",
    "fig_parcats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265629d4",
   "metadata": {},
   "source": [
    "# 2 Way interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f7897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_categories_plot(df_plot, dimensions_dct):\n",
    "    df_plot = df_plot.sort_values('importance', ascending=False)\n",
    "    fig_parcats = px.parallel_categories(\n",
    "        df_plot, \n",
    "        dimensions=dimensions_dct.keys(),\n",
    "        color=\"importance\",\n",
    "        color_continuous_scale=px.colors.sequential.Inferno,\n",
    "        labels=dimensions_dct,\n",
    "        range_color=[global_min, global_max]\n",
    "    )\n",
    "\n",
    "    fig_parcats.update_layout(\n",
    "        title=\"二維交互作用圖\",\n",
    "        height=800,\n",
    "        width=1500\n",
    "    )\n",
    "\n",
    "    fig_parcats.show()\n",
    "\n",
    "df_cause_road = df[(df['Vehicle'] == '無') & (df['Person'] != '無') & (df['Road'] != '無')].head(10).copy()\n",
    "df_cause_road['Interaction'] = '人 vs 路'\n",
    "df_car_road = df[(df['Vehicle'] != '無') & (df['Person'] == '無') & (df['Road'] != '無')].head(10).copy()\n",
    "df_car_road['Interaction'] = '車 vs 路'\n",
    "df_human_car = df[(df['Vehicle'] != '無') & (df['Person'] != '無') & (df['Road'] == '無')].head(10).copy()\n",
    "df_human_car['Interaction'] = '人 vs 車'\n",
    "\n",
    "df_combined = pd.concat([df_cause_road, df_car_road, df_human_car])\n",
    "\n",
    "# df_combined = df_combined[df_combined['importance'] > 0.001]\n",
    "\n",
    "df_combined = df_combined.sort_values('importance', ascending=False)\n",
    "\n",
    "dims = ['Interaction', 'Road', 'Vehicle', 'Person']\n",
    "labels_map = {'Interaction': '交互類型', 'Road':'道路設計', 'Vehicle':'車種', 'Person':'肇因'}\n",
    "# dims = ['Vehicle', 'Person']\n",
    "# labels_map = {'Vehicle':'車種', 'Person':'肇因'}\n",
    "\n",
    "fig_combined = px.parallel_categories(\n",
    "    df_combined, \n",
    "    dimensions=dims,\n",
    "    color=\"importance\",\n",
    "    color_continuous_scale=px.colors.sequential.Inferno,\n",
    "    labels=labels_map,\n",
    "    range_color=[global_min, global_max]\n",
    ")\n",
    "\n",
    "fig_combined.update_layout(\n",
    "    title=\"二維交互作用圖\",\n",
    "    height=900,\n",
    "    width=1600\n",
    ")\n",
    "\n",
    "fig_combined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f163233f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_data['車輛撞擊部位大類別名稱-最初'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91fc016",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data[(combined_data['車道劃分設施-分道設施-快車道或一般車道間名稱'] == '禁止變換車道線(無標記)') & \n",
    "              (combined_data['車輛撞擊部位大類別名稱-最初'] == '機車與自行車') &\n",
    "              (combined_data['cause_group'] == 'Posture') ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc4d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = '車道劃分設施-分道設施-快車道或一般車道間名稱_禁止變換車道線(無標記) x 車輛撞擊部位大類別名稱-最初_機車與自行車 x cause-group_Posture'\n",
    "a = X_train_refined[[col_name]]\n",
    "b = pd.concat([a, y_train], axis=1)\n",
    "\n",
    "b[col_name] = b[col_name].apply(lambda x: '有' if x > 0 else '無')\n",
    "b.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ST-RTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
