{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce50f96e",
   "metadata": {},
   "source": [
    "ComputedDataV3: Include facilities (youbike, parkinglot, mrt) <br/>\n",
    "ComputedDataV3: Remove facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20870616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "analyze_path = os.path.join(parent_dir, \"utils\")\n",
    "\n",
    "os.chdir(analyze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = ['Arial Unicode Ms']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, average_precision_score\n",
    "import torch\n",
    "from utils_nn import BinaryMLP, predict_nn\n",
    "from utils_model import model_preprocess, get_importance\n",
    "\n",
    "from utils import read_data\n",
    "from config import cause_mapping\n",
    "version = \"V1\"\n",
    "computeddata = 'ComputedDataV3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b971db",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = read_data()\n",
    "\n",
    "def map_cause(cause):\n",
    "    for category, causes in cause_mapping.items():\n",
    "        if cause in causes:\n",
    "            return category\n",
    "    return \"Unknown\"\n",
    "\n",
    "combined_data[\"cause_group\"] = combined_data[\"肇因研判子類別名稱-主要\"].apply(map_cause)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeec8b9f",
   "metadata": {},
   "source": [
    "本資料夾主要完成三個模型：\n",
    "- 模型1: (一維)人車路\n",
    "- 模型2: (二維)人車沒有路 (加法、乘法交互) (一維有路但沒交互&一維沒有路且沒交互)\n",
    "- 模型3: (三維)人車路 (加法、乘法交互)\n",
    "\n",
    "| 需要去掉市區道路(道路類別-第1當事者-名稱)、縣市(county)這種欄位，專注在道路設計（路）、肇因（人）、車種（車）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7521889",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_df = pd.read_csv(f\"../ComputedDataV2/ForModel/all_featuresV2.csv\")\n",
    "all_features_df = all_features_df[all_features_df.columns[~all_features_df.columns.str.contains('道路類別-第1當事者-名稱|county|路面狀況|道路障礙|車輛撞擊部位大類別名稱|original-speed')]]\n",
    "# all_features_df = all_features_df[all_features_df.columns[~all_features_df.columns.str.contains('道路類別-第1當事者-名稱|county|youbike|mrt|parkinglot')]] # for ComputedDataV4\n",
    "print(all_features_df.shape)\n",
    "\n",
    "road_features = [\n",
    "    '車道劃分設施-分道設施-快車道或一般車道間名稱',\n",
    "    '車道劃分設施-分道設施-快慢車道間名稱',\n",
    "    '車道劃分設施-分道設施-路面邊線名稱',\n",
    "    '車道劃分設施-分向設施大類別名稱',\n",
    "    '事故類型及型態大類別名稱',\n",
    "    '道路型態大類別名稱',\n",
    "    '號誌-號誌種類名稱',\n",
    "    '速限-第1當事者',\n",
    "    'youbike_100m_count',\n",
    "    'mrt_100m_count',\n",
    "    'parkinglot_100m_count',\n",
    "]\n",
    "car_features = [\n",
    "    '當事者區分-類別-大類別名稱-車種',\n",
    "]\n",
    "person_features = [\n",
    "    'cause-group',\n",
    "]\n",
    "\n",
    "# This is to check if all features are in the dataframe columns\n",
    "for feature in road_features + car_features + person_features:\n",
    "    match_found = any(feature in col for col in all_features_df.columns)\n",
    "    print(f\"Checking feature: {feature}, Found: {match_found}\")\n",
    "    \n",
    "    assert match_found, f\"Feature {feature} not found in dataframe columns\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb078fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "defined_keywords = road_features + car_features + person_features\n",
    "matched_cols = set()\n",
    "for col in all_features_df.columns:\n",
    "    for kw in defined_keywords:\n",
    "        if kw in col:\n",
    "            matched_cols.add(col)\n",
    "            break\n",
    "\n",
    "all_cols = set(all_features_df.columns)\n",
    "extra_cols = all_cols - matched_cols\n",
    "\n",
    "print(f\"Dataframe 總欄位數: {len(all_cols)}\")\n",
    "print(f\"已被關鍵字涵蓋的欄位數: {len(matched_cols)}\")\n",
    "print(f\"額外欄位數: {len(extra_cols)}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if len(extra_cols) > 0:\n",
    "    print(\"額外欄位:\")\n",
    "    for col in sorted(list(extra_cols)):\n",
    "        print(col)\n",
    "else:\n",
    "    print(\"所有欄位都已在列表\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f3ffe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_features_df.to_csv(f\"../ComputedDataV4/ForModel/all_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9151fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_model = {\n",
    "    'model_1': {\n",
    "        'grid_filter': None, # None is set to not include county data\n",
    "        'dim': '1way',\n",
    "        'base_road': road_features,\n",
    "        'base_vehicle': car_features,\n",
    "        'base_person': person_features,\n",
    "        'interaction_type': None,\n",
    "        'model': [],\n",
    "    },\n",
    "    'model_2_multiply_cp': {\n",
    "        'grid_filter': None,\n",
    "        'dim': '2way',\n",
    "        'base_road': None,\n",
    "        'base_vehicle': car_features,\n",
    "        'base_person': person_features,\n",
    "        'interaction_type': 'multiply',\n",
    "        'model': [],\n",
    "    },\n",
    "    'model_2_add_cp': {\n",
    "        'grid_filter': None,\n",
    "        'dim': '2way',\n",
    "        'base_road': None,\n",
    "        'base_vehicle': car_features,\n",
    "        'base_person': person_features,\n",
    "        'interaction_type': 'add',\n",
    "        'model': [],\n",
    "    },\n",
    "    'model_3_multiply': {\n",
    "        'grid_filter': None,\n",
    "        'dim': 'mixed',\n",
    "        'base_road': road_features,\n",
    "        'base_vehicle': car_features,\n",
    "        'base_person': person_features,\n",
    "        'interaction_type': 'multiply',\n",
    "        'model': [],\n",
    "    },\n",
    "    'model_3_add': {\n",
    "        'grid_filter': None,\n",
    "        'dim': 'mixed',\n",
    "        'base_road': road_features,\n",
    "        'base_vehicle': car_features,\n",
    "        'base_person': person_features,\n",
    "        'interaction_type': 'add',\n",
    "        'model': [],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d20e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extracted_features(lr, rf, X_resampled_test):\n",
    "    feature_order = lr.feature_names_in_\n",
    "    X_sorted = X_resampled_test.reindex(columns=feature_order, fill_value=0)\n",
    "\n",
    "    importance_lr_dict, _ = get_importance(lr, X_sorted)\n",
    "    df_lr_ranked = pd.DataFrame.from_dict(importance_lr_dict, orient='index', columns=['score', 'exp_score'])\n",
    "    df_lr_ranked['abs_score'] = df_lr_ranked['score'].abs()\n",
    "    importance_rf_dict, _ = get_importance(rf, X_sorted)\n",
    "    df_rf_ranked = pd.DataFrame.from_dict(importance_rf_dict, orient='index', columns=['score', 'exp_score'])\n",
    "    df_rf_ranked['abs_score'] = df_rf_ranked['score'].abs()\n",
    "\n",
    "    def get_dimension(feature_name):\n",
    "        x_count = feature_name.count(' x ')\n",
    "        if x_count == 0: return 1\n",
    "        if x_count == 1: return 2 \n",
    "        if x_count == 2: return 3\n",
    "        return 0\n",
    "\n",
    "    df_lr_ranked['dimension'] = df_lr_ranked.index.map(get_dimension)\n",
    "    df_rf_ranked['dimension'] = df_rf_ranked.index.map(get_dimension)\n",
    "\n",
    "    lr_dim1 = df_lr_ranked[df_lr_ranked['dimension'] == 1].index.tolist()\n",
    "    lr_dim2 = df_lr_ranked[df_lr_ranked['dimension'] == 2].sort_values('abs_score', ascending=False).head(20).index.tolist()\n",
    "    lr_dim3 = df_lr_ranked[df_lr_ranked['dimension'] == 3].sort_values('abs_score', ascending=False).head(20).index.tolist()\n",
    "    rf_dim1 = df_rf_ranked[df_rf_ranked['dimension'] == 1].index.tolist()\n",
    "    rf_dim2 = df_rf_ranked[df_rf_ranked['dimension'] == 2].sort_values('abs_score', ascending=False).head(20).index.tolist()\n",
    "    rf_dim3 = df_rf_ranked[df_rf_ranked['dimension'] == 3].sort_values('abs_score', ascending=False).head(20).index.tolist()\n",
    "\n",
    "    final_dim1 = list(set(lr_dim1 + rf_dim1))\n",
    "    final_dim2 = list(set(lr_dim2 + rf_dim2))\n",
    "    final_dim3 = list(set(lr_dim3 + rf_dim3))\n",
    "    final_feature_list = final_dim1 + final_dim2 + final_dim3\n",
    "\n",
    "    print(f\"Original feature: {len(final_dim1)}\")\n",
    "    print(f\"Two-way feature: {len(final_dim2)} (LR and RF union)\")\n",
    "    print(f\"Three-way feature: {len(final_dim3)} (LR and RF union)\")\n",
    "    print(f\"Final model feature total: {len(final_feature_list)}\")\n",
    "\n",
    "    print(\"top 3 way\")\n",
    "    print(df_lr_ranked[df_lr_ranked['dimension'] == 3].sort_values('abs_score', ascending=False).head(5).index.tolist())\n",
    "\n",
    "    return final_feature_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2322325",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "file_suffix_map = {\n",
    "    'model_1': 'model_1',\n",
    "    'model_2_add_cp': 'model_2_add_cp',\n",
    "    'model_2_multiply_cp': 'model_2_multiply_cp',\n",
    "    'model_3_add': 'model_3_add_extracted',\n",
    "    'model_3_multiply': 'model_3_multiply_extracted',\n",
    "}\n",
    "\n",
    "for model_name, config in dct_model.items():\n",
    "    print(f\"\\n=== Processing: {model_name} ===\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, X_resampled_test, y_resampled_test, le = model_preprocess(\n",
    "        all_features_df, \n",
    "        grid_filter=config['grid_filter'],\n",
    "        dim=config['dim'],\n",
    "        base_road=config['base_road'],\n",
    "        base_vehicle=config['base_vehicle'],\n",
    "        base_person=config['base_person'],\n",
    "        interaction_type=config['interaction_type'],\n",
    "    )\n",
    "\n",
    "    suffix = file_suffix_map.get(model_name, model_name) \n",
    "    base_path = f'../{computeddata}/ModelPerformance'\n",
    "\n",
    "    if model_name in ['model_3_add', 'model_3_multiply']:\n",
    "        lr = joblib.load(f'../{computeddata}/ModelPerformance/lr_modelV1_{model_name}.pkl')\n",
    "        rf = joblib.load(f'../{computeddata}/ModelPerformance/rf_modelV1_{model_name}.pkl')\n",
    "        final_feature_list = get_extracted_features(lr, rf, X_resampled_test)    \n",
    "        X_train_refined = X_train[final_feature_list]\n",
    "    # X_test_refined = X_resampled_test[final_feature_list]\n",
    "    try:\n",
    "\n",
    "        lr_path = f'{base_path}/lr_modelV1_{suffix}.pkl'\n",
    "        lr_model = joblib.load(lr_path)\n",
    "        print(f\"  -> Loaded LR: {lr_path}\")\n",
    "\n",
    "        rf_path = f'{base_path}/rf_modelV1_{suffix}.pkl'\n",
    "        rf_model = joblib.load(rf_path)\n",
    "        print(f\"  -> Loaded RF: {rf_path}\")\n",
    "\n",
    "        if model_name in ['model_3_add', 'model_3_multiply']:\n",
    "            input_dim = X_train_refined.shape[1]\n",
    "            print(f\"  -> Using refined input dim: {input_dim}\")\n",
    "        else:\n",
    "            input_dim = X_train.shape[1]\n",
    "            print(f\"  -> Using input dim: {input_dim}\")\n",
    "        nn_model = BinaryMLP(in_dim=input_dim).to(device)\n",
    "        nn_path = f'{base_path}/nn_modelV1_{suffix}.pt'\n",
    "        nn_model.load_state_dict(torch.load(nn_path, map_location=device))\n",
    "        print(f\"  -> Loaded NN: {nn_path} (dim={input_dim})\")\n",
    "\n",
    "        config['model'] = [lr_model, rf_model, nn_model]\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\" [ERROR] File not found: {e}\")\n",
    "        continue "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcbdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re fit for only important features\n",
    "def print_results(proba_test, classes, y_resampled_test):\n",
    "    \"\"\"\n",
    "    proba_test: 預測的概率\n",
    "    classes: 類別名稱\n",
    "    y_resampled_test: 重抽樣後的測試標籤\n",
    "    \"\"\"\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    y_pred = np.argmax(proba_test, axis=1)\n",
    "\n",
    "    print(\"Confusion Matrix\")\n",
    "    conf_matrix = confusion_matrix(y_resampled_test, y_pred, labels=range(len(classes)))\n",
    "    print(conf_matrix)\n",
    "\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(\n",
    "        y_resampled_test, y_pred, target_names=classes, digits=3\n",
    "    ))\n",
    "\n",
    "    if proba_test.shape[1] == 2:\n",
    "        # 二元分類\n",
    "        roc_auc = roc_auc_score(y_resampled_test, proba_test[:, 1])\n",
    "        print(f'ROC AUC: {roc_auc:.3f}')\n",
    "        y_test_bin = label_binarize(y_resampled_test, classes=range(len(classes)))\n",
    "        pr_auc_macro  = average_precision_score(y_test_bin, proba_test[:, 1], average='macro')\n",
    "        pr_auc_weight = average_precision_score(y_test_bin, proba_test[:, 1], average='weighted')\n",
    "        print(f'PR  AUC macro: {pr_auc_macro:.3f}')\n",
    "        print(f'PR  AUC wighted: {pr_auc_weight:.3f}')\n",
    "\n",
    "    return conf_matrix\n",
    "\n",
    "all_performance = {}\n",
    "all_importance = {}\n",
    "for model_name, config in dct_model.items():\n",
    "\n",
    "    print(f\"training model: {model_name}\")\n",
    "    X_train, X_test, y_train, y_test, X_resampled_test, y_resampled_test, le = model_preprocess(\n",
    "        all_features_df, \n",
    "        grid_filter=config['grid_filter'],\n",
    "        dim=config['dim'],\n",
    "        base_road=config['base_road'],\n",
    "        base_vehicle=config['base_vehicle'],\n",
    "        base_person=config['base_person'],\n",
    "        interaction_type=config['interaction_type'],\n",
    "    )\n",
    "    # don't need two reorder because they are using the same columns in training\n",
    "    feature_order_lr = config['model'][0].feature_names_in_\n",
    "\n",
    "    if model_name == 'model_3_add' or model_name == 'model_3_multiply':\n",
    "        final_feature_list = get_extracted_features(config['model'][0], config['model'][1], X_resampled_test)\n",
    "        X_resampled_test = X_resampled_test[final_feature_list]\n",
    "\n",
    "    X_sorted = X_resampled_test.reindex(columns=feature_order_lr, fill_value=0)\n",
    "    X_resampled_reordered = X_sorted.reindex(columns=feature_order_lr, fill_value=0)\n",
    "\n",
    "    proba_test_lr = config['model'][0].predict_proba(X_resampled_reordered)\n",
    "    proba_test_rf = config['model'][1].predict_proba(X_resampled_reordered)\n",
    "    proba_test_nn = predict_nn(config['model'][2], X_resampled_reordered)\n",
    "    \n",
    "    # print('model performance for lr')\n",
    "    # print_results(proba_test_lr, le.classes_, y_resampled_test)\n",
    "    # print('model performance for rf')\n",
    "    # print_results(proba_test_rf, le.classes_, y_resampled_test)\n",
    "\n",
    "    all_performance[model_name] = {\n",
    "        'lr': print_results(proba_test_lr, le.classes_, y_resampled_test),\n",
    "        'rf': print_results(proba_test_rf, le.classes_, y_resampled_test),\n",
    "        'nn': print_results(proba_test_nn, le.classes_, y_resampled_test),\n",
    "    }\n",
    "\n",
    "    importance_lr, importance_grouped_lr = get_importance(config['model'][0], X_resampled_reordered)\n",
    "    importance_rf, importance_grouped_rf = get_importance(config['model'][1], X_resampled_reordered)\n",
    "\n",
    "    all_importance[model_name] = {\n",
    "        'lr': [importance_lr, importance_grouped_lr],\n",
    "        'rf': [importance_rf, importance_grouped_rf],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3095fb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, X_resampled_test, y_resampled_test, le = model_preprocess(\n",
    "    all_features_df, \n",
    "    grid_filter=dct_model['model_3_multiply']['grid_filter'],\n",
    "    dim=dct_model['model_3_multiply']['dim'],\n",
    "    base_road=dct_model['model_3_multiply']['base_road'],\n",
    "    base_vehicle=dct_model['model_3_multiply']['base_vehicle'],\n",
    "    base_person=dct_model['model_3_multiply']['base_person'],\n",
    "    interaction_type=dct_model['model_3_multiply']['interaction_type'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28fa2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry import box\n",
    "from shapely import wkt\n",
    "import ast\n",
    "TM2 = 3826\n",
    "\n",
    "def read_taiwan_specific(read_grid=False):\n",
    "    taiwan = gpd.read_file('../Data/OFiles_9e222fea-bafb-4436-9b17-10921abc6ef2/TOWN_MOI_1140318.shp')\n",
    "    taiwan = taiwan[(~taiwan['TOWNNAME'].isin(['蘭嶼鄉', '綠島鄉', '琉球鄉'])) & \n",
    "                    (~taiwan['COUNTYNAME'].isin(['金門縣', '連江縣', '澎湖縣']))].to_crs(TM2)\n",
    "\n",
    "    minx, miny, maxx, maxy = taiwan.total_bounds\n",
    "    clip_box = box(minx, 2400000, 380000, maxy)\n",
    "    clipper = gpd.GeoDataFrame(geometry=[clip_box], crs=taiwan.crs)\n",
    "    taiwan = gpd.clip(taiwan, clipper)\n",
    "\n",
    "    if read_grid:\n",
    "        taiwan_cnty = taiwan[['COUNTYNAME','geometry']].dissolve(by='COUNTYNAME')\n",
    "        taiwan_cnty['geometry'] = taiwan_cnty.buffer(0)\n",
    "\n",
    "        # 原始以 0.001 grid 計算出的區域事故及對應索引, 依照 hex_grid 計算出來的GI\n",
    "        grid_gi_df = pd.read_csv('../ComputedDataV2/Grid/grid_giV1.csv')\n",
    "        grid_gi_df['accident_indices'] = grid_gi_df['accident_indices'].apply(ast.literal_eval)\n",
    "        grid_gi_df['geometry'] = grid_gi_df['geometry'].apply(wkt.loads)\n",
    "        grid_gi  = gpd.GeoDataFrame(grid_gi_df, geometry='geometry').set_crs(TM2, allow_override=True)\n",
    "        grid_gi['geometry'] = grid_gi.geometry#.centroid\n",
    "\n",
    "        county_join = gpd.sjoin(grid_gi[['geometry']], taiwan_cnty, how='left', predicate='within')\n",
    "        grid_gi['COUNTYNAME'] = county_join['COUNTYNAME']\n",
    "        # 這些都是離島資料，因為在taiwan被篩選掉了，所以會因為對應不到所以回傳空值\n",
    "        grid_filter = grid_gi[grid_gi['accident_indices'].str.len() > 0]\n",
    "        grid_filter.reset_index(inplace=True)\n",
    "\n",
    "    else:\n",
    "        grid_filter = None\n",
    "\n",
    "    return taiwan, grid_filter\n",
    "\n",
    "taiwan, grid_filter = read_taiwan_specific(read_grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfb4cb4",
   "metadata": {},
   "source": [
    "## Use proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d118062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "full_interaction_df = pd.concat([X_train, X_test], ignore_index=True)\n",
    "\n",
    "choose_model = dct_model['model_3_multiply']['model'][1]\n",
    "fdata = full_interaction_df.reindex(columns=choose_model.feature_names_in_, fill_value=0)\n",
    "proba_test_rf = choose_model.predict_proba(fdata)\n",
    "\n",
    "# join on index\n",
    "all_features_gdf = full_interaction_df.join(\n",
    "    grid_filter[['geometry', 'COUNTYNAME', 'hotspot']], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "all_features_gdf = gpd.GeoDataFrame(\n",
    "    all_features_gdf, \n",
    "    geometry='geometry', \n",
    "    crs=grid_filter.crs \n",
    ")\n",
    "\n",
    "all_features_gdf['risk_prob'] = proba_test_rf[:, 1]\n",
    "all_features_gdf['hotspot_binary_true'] = all_features_gdf['hotspot'].apply(lambda x: 0 if x == 'Not Significant' else 1)\n",
    "all_features_gdf['risk_binary'] = all_features_gdf['risk_prob'].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "\n",
    "print(classification_report(all_features_gdf['hotspot_binary_true'], all_features_gdf['risk_binary']))\n",
    "\n",
    "print(all_features_gdf[['risk_binary', 'hotspot_binary_true']].value_counts(normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd5fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily as ctx\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def plot_hotspot_map(gdf_plot, target_feature, binary=False):\n",
    "    gdf_plot = gdf_plot.to_crs(epsg=3857)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 15), dpi=150)\n",
    "\n",
    "    if binary:\n",
    "        gdf_plot.plot(\n",
    "            ax=ax,\n",
    "            color='red', \n",
    "            alpha=0.5, \n",
    "            edgecolor='none', \n",
    "            legend=False\n",
    "        )\n",
    "    else:\n",
    "        gdf_plot.plot(\n",
    "            column=target_feature,\n",
    "            ax=ax,\n",
    "            cmap='YlOrRd',\n",
    "            cax=make_axes_locatable(ax).append_axes(\"bottom\", size=\"3%\", pad=0.1),\n",
    "            alpha=0.5,\n",
    "            legend=True,\n",
    "            edgecolor='none',\n",
    "            legend_kwds={'label': \"Risk\", 'orientation': \"horizontal\", 'shrink': 0.6}\n",
    "        )\n",
    "\n",
    "\n",
    "    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    # output_img = \"risk_map_static.png\"\n",
    "    # plt.savefig(output_img, bbox_inches='tight', pad_inches=0.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_feature = '車道劃分設施-分道設施-路面邊線名稱_無 x 當事者區分-類別-大類別名稱-車種_小客車(含客、貨兩用) x cause-group_Decision'\n",
    "target_feature = 'risk_prob'\n",
    "# target_feature = 'hotspot_binary_true'\n",
    "\n",
    "gdf_plot = all_features_gdf[\n",
    "    (all_features_gdf[target_feature] > 0) & \n",
    "    (all_features_gdf['COUNTYNAME'] == '臺中市')\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71904ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_hotspot_map(gdf_plot, target_feature, binary=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ST-RTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
