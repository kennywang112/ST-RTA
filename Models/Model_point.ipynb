{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e886fea1",
   "metadata": {},
   "source": [
    "該版本與Model不同在於它將回推事故實際點否為熱點，單位是單一資料而非整個grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf18b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "analyze_path = os.path.join(parent_dir, \"utils\")\n",
    "\n",
    "os.chdir(analyze_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27174a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = ['Arial Unicode Ms']\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "from utils_model import eval_loop, to_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7df2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_data\n",
    "combined_data = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "import geopandas as gpd\n",
    "import ast\n",
    "\n",
    "TM2 = 3826\n",
    "taiwan = gpd.read_file('../Data/OFiles_9e222fea-bafb-4436-9b17-10921abc6ef2/TOWN_MOI_1140318.shp')\n",
    "taiwan = taiwan[(~taiwan['TOWNNAME'].isin(['旗津區', '頭城鎮', '蘭嶼鄉', '綠島鄉', '琉球鄉'])) & \n",
    "                (~taiwan['COUNTYNAME'].isin(['金門縣', '連江縣', '澎湖縣']))].to_crs(TM2)\n",
    "taiwan_cnty = taiwan[['COUNTYNAME','geometry']].dissolve(by='COUNTYNAME')\n",
    "taiwan_cnty['geometry'] = taiwan_cnty.buffer(0)\n",
    "\n",
    "# 原始以 0.001 grid 計算出的區域事故及對應索引\n",
    "hex_grid_raw = pd.read_csv('../ComputedData/Grid/hex_grid.csv')\n",
    "hex_grid_raw['geometry'] = hex_grid_raw['geometry'].apply(wkt.loads)\n",
    "hex_grid = gpd.GeoDataFrame(hex_grid_raw, geometry='geometry').set_crs(TM2, allow_override=True)\n",
    "hex_grid['geometry'] = hex_grid.geometry.centroid\n",
    "\n",
    "# 依照 hex_grid 計算出來的GI\n",
    "grid_gi_df = pd.read_csv('../ComputedData/Grid/grid_gi.csv')\n",
    "grid_gi_df['accident_indices'] = grid_gi_df['accident_indices'].apply(ast.literal_eval)\n",
    "grid_gi_df['geometry'] = grid_gi_df['geometry'].apply(wkt.loads)\n",
    "grid_gi  = gpd.GeoDataFrame(grid_gi_df, geometry='geometry').set_crs(TM2, allow_override=True)\n",
    "\n",
    "county_join = gpd.sjoin(hex_grid[['geometry']], taiwan_cnty, how='left', predicate='within')\n",
    "grid_gi['COUNTYNAME'] = county_join['COUNTYNAME']\n",
    "\n",
    "print('NaN ratio:', county_join['COUNTYNAME'].isna().mean())\n",
    "\n",
    "grid_gi['hotspot'] = grid_gi['hotspot'].apply(lambda x: 'Hotspot' if 'Hotspot' in str(x) else 'Not Hotspot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed2369",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in grid_gi.iterrows():\n",
    "    accidents = row['accident_indices']\n",
    "    if row['hotspot'] == 'Hotspot':\n",
    "        combined_data.loc[accidents, 'hotspot'] = 'Hotspot'\n",
    "        combined_data.loc[accidents, 'COUNTYNAME'] = row['COUNTYNAME']\n",
    "    else:\n",
    "        combined_data.loc[accidents, 'hotspot'] = 'Not Hotspot'\n",
    "        combined_data.loc[accidents, 'COUNTYNAME'] = row['COUNTYNAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e452fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = combined_data[combined_data['COUNTYNAME'].notna()]\n",
    "combined_data.drop(columns='num_accidents', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c550ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import select_group\n",
    "\n",
    "select_group = [\n",
    "    # 氣候暫不討論\n",
    "    # '天候名稱', '光線名稱',\n",
    "\n",
    "    # 道路問題\n",
    "    '路面狀況-路面鋪裝名稱', '路面狀況-路面狀態名稱', '路面狀況-路面缺陷名稱',\n",
    "    '道路障礙-障礙物名稱', '道路障礙-視距品質名稱', '道路障礙-視距名稱',\n",
    "\n",
    "    # 號誌\n",
    "    '號誌-號誌種類名稱', '號誌-號誌動作名稱',\n",
    "\n",
    "    # 車道劃分\n",
    "    '車道劃分設施-分道設施-快車道或一般車道間名稱',\n",
    "    '車道劃分設施-分道設施-快慢車道間名稱', '車道劃分設施-分道設施-路面邊線名稱',\n",
    "\n",
    "    # 大類別\n",
    "    # '肇因研判大類別名稱-主要', '肇因研判大類別名稱-個別', # 聚焦道路類型\n",
    "    # '當事者區分-類別-大類別名稱-車種', # 聚焦道路類型\n",
    "    # '當事者行動狀態大類別名稱', # 聚焦道路類型\n",
    "    '車輛撞擊部位大類別名稱-最初', #'車輛撞擊部位大類別名稱-其他',\n",
    "    '事故類型及型態大類別名稱', '車道劃分設施-分向設施大類別名稱',\n",
    "    '事故位置大類別名稱', '道路型態大類別名稱',\n",
    "    \n",
    "    # 子類別\n",
    "    # '肇因研判子類別名稱-主要', '肇因研判子類別名稱-個別', # 聚焦道路類型\n",
    "    # '當事者區分-類別-子類別名稱-車種', # 聚焦道路類型\n",
    "    # '當事者行動狀態子類別名稱', # 聚焦道路類型\n",
    "    # '車輛撞擊部位子類別名稱-最初', '車輛撞擊部位子類別名稱-其他', # 道路類型很大程度影響撞擊部位，所以不考慮\n",
    "    # '事故類型及型態子類別名稱', '車道劃分設施-分向設施子類別名稱', \n",
    "    # '事故位置子類別名稱', '道路型態子類別名稱',\n",
    "\n",
    "    # 其他\n",
    "    # '當事者屬-性-別名稱', '當事者事故發生時年齡', \n",
    "    '速限-第1當事者', '道路類別-第1當事者-名稱',\n",
    "    # '保護裝備名稱', '行動電話或電腦或其他相類功能裝置名稱', '肇事逃逸類別名稱-是否肇逃',\n",
    "\n",
    "    # 設施\n",
    "    'youbike_100m_count', 'mrt_100m_count', 'parkinglot_100m_count',\n",
    "\n",
    "    # A1 or A2\n",
    "    # 'source',\n",
    "    ]\n",
    "\n",
    "select_group.append('COUNTYNAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1b2ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_df = pd.get_dummies(combined_data[select_group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9f6382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with county town\n",
    "# 原始資料index並非從1開始所以需reset\n",
    "new_grid_encoded = all_features_df\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(combined_data['hotspot'])\n",
    "X = new_grid_encoded\n",
    "\n",
    "# interaction\n",
    "from utils_model import get_interaction\n",
    "X = get_interaction(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "y_train = pd.Series(y_train, index=X_train.index)\n",
    "y_test  = pd.Series(y_test,  index=X_test.index)\n",
    "\n",
    "# undersampling\n",
    "cls_counts = y_test.value_counts()\n",
    "min_count = cls_counts.min()\n",
    "rus_test = RandomUnderSampler(\n",
    "    sampling_strategy={int(c): int(min_count) for c in cls_counts.index},\n",
    "    random_state=42\n",
    ")\n",
    "X_resampled_test, y_resampled_test = rus_test.fit_resample(X_test, y_test)\n",
    "\n",
    "print(\"before US\")\n",
    "print(pd.Series(y_test).map(dict(enumerate(le.classes_))).value_counts())\n",
    "print(\"after US\")\n",
    "print(pd.Series(y_resampled_test).map(dict(enumerate(le.classes_))).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914ac430",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "        penalty='elasticnet', solver='saga', l1_ratio=0.5,\n",
    "        class_weight='balanced', max_iter=1000, \n",
    "        random_state=42, \n",
    "        multi_class='multinomial',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, clf in [('Logistic', lr)]:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv, n_jobs=-1,\n",
    "                            # scoring='roc_auc_ovr_weighted',\n",
    "                            scoring='roc_auc'\n",
    "                             )\n",
    "    print(f'{name} CV ROC AUC: {scores.mean():.3f} ± {scores.std():.3f}')\n",
    "\n",
    "proba_test_lr = lr.predict_proba(X_resampled_test)\n",
    "y_pred_lr = np.argmax(proba_test_lr, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8166bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "INPUT_DIM = X_resampled_test.shape[1]\n",
    "NUM_CLASSES = int(len(set(y)))  # 類別 0/1\n",
    "\n",
    "class BinaryMLP(nn.Module):\n",
    "    def __init__(self, in_dim=INPUT_DIM, num_classes=NUM_CLASSES, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(64, num_classes)  # logits\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf88ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nn, X_val_nn, y_train_nn, y_val_nn = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "X_train_t, y_train_t = to_tensors(X_train, y_train)\n",
    "X_val_t, y_val_t = to_tensors(X_val_nn, y_val_nn)\n",
    "X_test_t, y_test_t = to_tensors(X_resampled_test, y_resampled_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=256, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=512, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=512, shuffle=False)\n",
    "\n",
    "model = BinaryMLP().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val = -np.inf\n",
    "patience = 5\n",
    "wait = 0\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2bbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    val_metrics = eval_loop(model, test_loader, le)\n",
    "    print(f'Epoch {epoch:02d}/{epochs} | loss {train_loss:.4f} | '\n",
    "          f'val_acc {val_metrics[\"acc\"]:.3f} | val_f1 {val_metrics[\"f1\"]:.3f} | val_auc {val_metrics[\"auc\"]:.3f}')\n",
    "\n",
    "    score_for_early = val_metrics[\"auc\"]  # 你也可用 f1\n",
    "    if score_for_early > best_val:\n",
    "        best_val = score_for_early\n",
    "        wait = 0\n",
    "        # torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print('Early stopping.')\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ST-RTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
