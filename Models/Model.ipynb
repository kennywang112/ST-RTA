{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eeb03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "analyze_path = os.path.join(parent_dir, \"utils\")\n",
    "\n",
    "os.chdir(analyze_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1b50e",
   "metadata": {},
   "source": [
    "要先建立輸入到模型的資料\n",
    "- 若是要分類是否是熱點，應該要以一個區域的grid為單位\n",
    "- 所以建立得grid亦包含該地區的所有特徵資料，以比例顯示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a2ccdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = ['Arial Unicode Ms']\n",
    "# plt.rcParams['font.sans-serif'] = ['Microsoft YaHei']\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, recall_score, precision_score\n",
    "from utils_model import eval_loop, to_tensors\n",
    "from utils import read_data, read_taiwan_specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19c72a",
   "metadata": {},
   "source": [
    "## obtain hotspot's county\n",
    "這段使用將grid依照geometry找出個別在哪個城市"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de7952c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = read_data()\n",
    "taiwan, grid_filter = read_taiwan_specific(read_grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7638c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_d_old = pd.read_csv(\"../ComputedData/ForModel/all_featuresV2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b478ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_featuresV2 為將離群替換為中位數\n",
    "all_features_df = pd.read_csv(\"../ComputedDataV2/ForModel/all_featuresV1.csv\")\n",
    "\n",
    "# 移除高共線\n",
    "cols = all_features_df.columns[all_features_df.columns.str.contains('事故位置大類別名稱')]\n",
    "cols2 = all_features_df.columns[all_features_df.columns.str.contains('號誌動作')]\n",
    "all_features_df.drop(columns=cols, inplace=True)\n",
    "all_features_df.drop(columns=cols2, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7ccea5",
   "metadata": {},
   "source": [
    "### Model Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c96d0074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangqiqian/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/sklearn/base.py:484: FutureWarning: `BaseEstimator._check_n_features` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_n_features` instead.\n",
      "  warnings.warn(\n",
      "/Users/wangqiqian/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/sklearn/base.py:493: FutureWarning: `BaseEstimator._check_feature_names` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation._check_feature_names` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# with county town\n",
    "# 原始資料index並非從1開始所以需reset\n",
    "new_grid = pd.concat([grid_filter[['COUNTYNAME']], all_features_df], axis=1)\n",
    "county_dummies = pd.get_dummies(new_grid['COUNTYNAME'], prefix='county')\n",
    "new_grid_encoded = pd.concat([new_grid.drop(['COUNTYNAME'], axis=1), county_dummies], axis=1)\n",
    "\n",
    "# binary hotspot\n",
    "new_grid_encoded['hotspot'] = new_grid_encoded['hotspot'].apply(lambda x: 'Hotspot' if 'Hotspot' in str(x) else 'Not Hotspot')\n",
    "le = LabelEncoder()\n",
    "# y = le.fit_transform(new_grid_encoded['hotspot'])\n",
    "y = new_grid_encoded['hotspot'].map({'Not Hotspot': 0, 'Hotspot': 1}).values\n",
    "X = new_grid_encoded.drop(columns=['hotspot'])\n",
    "\n",
    "# interaction\n",
    "from utils_model import get_interaction\n",
    "X = get_interaction(X)\n",
    "\n",
    "X.drop(columns='original_speed', inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "y_train = pd.Series(y_train, index=X_train.index)\n",
    "y_test  = pd.Series(y_test,  index=X_test.index)\n",
    "\n",
    "# undersampling\n",
    "cls_counts = y_test.value_counts()\n",
    "min_count = cls_counts.min()\n",
    "rus_test = RandomUnderSampler(\n",
    "    sampling_strategy={int(c): int(min_count) for c in cls_counts.index},\n",
    "    random_state=42\n",
    ")\n",
    "X_resampled_test, y_resampled_test = rus_test.fit_resample(X_test, y_test)\n",
    "\n",
    "# print(\"before US\")\n",
    "# print(pd.Series(y_test).map(dict(enumerate(le.classes_))).value_counts())\n",
    "# print(\"after US\")\n",
    "# print(pd.Series(y_resampled_test).map(dict(enumerate(le.classes_))).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cbf8b3",
   "metadata": {},
   "source": [
    "# LR and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab900861",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_lr = {\n",
    "    'penalty': ['elasticnet'],\n",
    "    'solver': ['saga'],\n",
    "    'l1_ratio': [0.1, 0.5, 0.9],\n",
    "    'C': [0.01, 0.1, 1, 10]\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a0b4110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wangqiqian/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/wangqiqian/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/wangqiqian/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/wangqiqian/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/wangqiqian/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m\n\u001b[1;32m     29\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[1;32m     30\u001b[0m         n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     31\u001b[0m         class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, clf \u001b[38;5;129;01min\u001b[39;00m [(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogistic\u001b[39m\u001b[38;5;124m'\u001b[39m, lr), (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandomForest\u001b[39m\u001b[38;5;124m'\u001b[39m, rf)]:\n\u001b[0;32m---> 35\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# scoring='roc_auc_ovr_weighted',\u001b[39;49;00m\n\u001b[1;32m     37\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroc_auc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     38\u001b[0m \u001b[43m                             \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m CV ROC AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscores\u001b[38;5;241m.\u001b[39mstd()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m lr\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:684\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    682\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 684\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    214\u001b[0m         )\n\u001b[1;32m    215\u001b[0m     ):\n\u001b[0;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    226\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:411\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    410\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 411\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    431\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/joblib/parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/joblib/parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ST-RTA/lib/python3.9/site-packages/joblib/parallel.py:1800\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_ordered:\n\u001b[1;32m   1790\u001b[0m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1795\u001b[0m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[1;32m   1796\u001b[0m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[1;32m   1797\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1798\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING\n\u001b[1;32m   1799\u001b[0m     ):\n\u001b[0;32m-> 1800\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1801\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1811\u001b[0m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[1;32m   1812\u001b[0m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lr = GridSearchCV(\n",
    "#     LogisticRegression(\n",
    "#         penalty='elasticnet', solver='saga',\n",
    "#         class_weight='balanced', max_iter=1000, \n",
    "#         random_state=42, multi_class='multinomial'\n",
    "#     ),\n",
    "#     param_grid=param_grid_lr,\n",
    "#     cv=cv,\n",
    "#     scoring='roc_auc',\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# rf = GridSearchCV(\n",
    "#     RandomForestClassifier(\n",
    "#         class_weight='balanced', random_state=42\n",
    "#     ),\n",
    "#     param_grid=param_grid_rf,\n",
    "#     cv=cv,\n",
    "#     scoring='roc_auc',\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "\n",
    "lr = LogisticRegression(\n",
    "        penalty='elasticnet', solver='saga', l1_ratio=0.5,\n",
    "        class_weight='balanced', max_iter=1000, \n",
    "        random_state=42, \n",
    "        multi_class='multinomial',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "rf = RandomForestClassifier(\n",
    "        n_estimators=300, max_depth=None, min_samples_leaf=1,\n",
    "        class_weight='balanced', n_jobs=-1, random_state=42,\n",
    "    )\n",
    "\n",
    "for name, clf in [('Logistic', lr), ('RandomForest', rf)]:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv, n_jobs=-1,\n",
    "                            # scoring='roc_auc_ovr_weighted',\n",
    "                            scoring='roc_auc'\n",
    "                             )\n",
    "    print(f'{name} CV ROC AUC: {scores.mean():.3f} ± {scores.std():.3f}')\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "proba_test_lr = lr.predict_proba(X_resampled_test)\n",
    "proba_test_rf = rf.predict_proba(X_resampled_test)\n",
    "y_pred_lr = np.argmax(proba_test_lr, axis=1)\n",
    "y_pred_rf = np.argmax(proba_test_rf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd47588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# joblib.dump(lr, '../ComputedData/ModelPerformance/lr_modelV3.pkl')\n",
    "# joblib.dump(rf, '../ComputedData/ModelPerformance/rf_modelV3.pkl')\n",
    "\n",
    "# load model\n",
    "lr = joblib.load('../ComputedData/ModelPerformance/lr_modelV3.pkl')\n",
    "rf = joblib.load('../ComputedData/ModelPerformance/rf_modelV3.pkl')\n",
    "\n",
    "proba_test_lr = lr.predict_proba(X_resampled_test)\n",
    "proba_test_rf = rf.predict_proba(X_resampled_test)\n",
    "y_pred_lr = np.argmax(proba_test_lr, axis=1)\n",
    "y_pred_rf = np.argmax(proba_test_rf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a66d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_model import print_results\n",
    "\n",
    "print_results(proba_test_lr, le.classes_, y_resampled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da476ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(proba_test_lr, ['Not Hotspot', 'Hotspot'], y_resampled_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6017ce8",
   "metadata": {},
   "source": [
    "設施平均：該地區的事故點附近平均會有幾個設施"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0bbf5",
   "metadata": {},
   "source": [
    "### RandomForest Feature Importance & LinearRegression coefficient\n",
    "- group的寫法可能還要再修"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855c8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_importance(model, df, specific_col=None):\n",
    "    if model.__class__.__name__ == 'LogisticRegression':\n",
    "        # coefs = model.coef_[0]\n",
    "        # importances = np.abs(coefs)\n",
    "        importances = model.coef_[0]\n",
    "    else:\n",
    "        importances = model.feature_importances_\n",
    "\n",
    "    feature_names = df.columns\n",
    "\n",
    "    if specific_col:\n",
    "        sel_idx = [i for i, name in enumerate(feature_names) if specific_col in name]\n",
    "        indices = np.argsort(importances[sel_idx])[::-1]\n",
    "        indices = [sel_idx[i] for i in indices] # 對應回原始 index\n",
    "    else:\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # 整合整個欄位為一個重要性\n",
    "    importance_ungrouped = {}\n",
    "    for i in indices:\n",
    "        # 直接group importance不可做exp，他並不代表勝算比\n",
    "        importance_ungrouped[feature_names[i]] = [importances[i], np.exp(importances[i])]\n",
    "        # importance_ungrouped[feature_names[i]] = [importances[i], importances[i]]\n",
    "\n",
    "    fi_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "    fi_df['main_feature'] = fi_df['feature'].str.split('_').str[0]\n",
    "    grouped = fi_df.groupby('main_feature', as_index=False)['importance'].sum()\n",
    "    grouped['exp'] = np.exp(grouped['importance'])\n",
    "    grouped = grouped.sort_values('importance', ascending=False)\n",
    "\n",
    "    return importance_ungrouped, grouped\n",
    "\n",
    "importance_rf, importance_grouped_rf = get_importance(rf, X_train)\n",
    "importance_lr, importance_grouped_lr = get_importance(lr, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c60433",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_structured_county(importance):\n",
    "    \"\"\"\n",
    "    importance: dict, 例如 {\"特徵A x county_臺北市\": [coef, OR], ...}\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "\n",
    "    for name, v in importance.items():\n",
    "        # 跳過純縣市 dummy，如 \"county_臺北市\"\n",
    "        if name.startswith(\"county_\") and \" x county_\" not in name:\n",
    "            continue\n",
    "        # 只處理有交互字樣的\n",
    "        if \" x county_\" not in name:\n",
    "            continue\n",
    "\n",
    "        base, county_tag = name.split(\" x county_\", 1)  # base 特徵名、county_tag 縣市名（不含前綴）\n",
    "\n",
    "        out.setdefault(county_tag, {})[base] = v[1]\n",
    "\n",
    "    return out\n",
    "\n",
    "structured_all = build_structured_county(importance_lr)\n",
    "structured_all = pd.DataFrame(structured_all)\n",
    "structured_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c6011c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import col_translation, countycity_dct\n",
    "odds_df = structured_all[['臺北市', '新北市', '臺中市', '高雄市', '臺東縣', '花蓮縣']]\n",
    "\n",
    "odds_df.sort_values(by='臺北市', ascending=False)\n",
    "odds_df = odds_df.rename(columns=countycity_dct, index=col_translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27cdd8f",
   "metadata": {},
   "source": [
    "## Odds Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b263745",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = ['Arial Unicode Ms']\n",
    "\n",
    "df_long = odds_df.stack().reset_index()\n",
    "df_long.columns = ['feature', 'county', 'value']\n",
    "\n",
    "# 用 max|ln(OR)| 當排序依據，抓前 top_k 個以免圖太擁擠\n",
    "# 用 OR 排序會忽略小於 1 的強負向特徵\n",
    "top_k = 30\n",
    "rank = (\n",
    "    df_long.assign(abs_log=lambda d: np.abs(np.log(d['value'])))\n",
    "           .groupby('feature')['abs_log'].max()\n",
    "           .sort_values(ascending=False)\n",
    ")\n",
    "feat_sel = rank.head(top_k).index\n",
    "plot_data = df_long[df_long['feature'].isin(feat_sel)]\n",
    "\n",
    "features = plot_data['feature'].dropna().unique().tolist()\n",
    "features = [f for f in rank.index if f in features]  # 用上面 rank 的順序\n",
    "ypos = {f: i for i, f in enumerate(features)}\n",
    "\n",
    "plt.figure(figsize=(14, max(6, 0.4*len(features))))\n",
    "plt.axvline(1.0, linestyle='--', linewidth=1)\n",
    "plt.axvline(1.5, linestyle='--', linewidth=1)\n",
    "plt.axvline(0.5, linestyle='--', linewidth=1)\n",
    "plt.axvline(2, linestyle='--', linewidth=1)\n",
    "\n",
    "for county, d in plot_data.groupby('county'):\n",
    "    y = [ypos[f] for f in d['feature']]\n",
    "    plt.scatter(d['value'], y, s=20, label=county)\n",
    "\n",
    "plt.yticks(range(len(features)), features)\n",
    "plt.xlabel('Odds Ratio (OR)')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Counties effect per feature (x = OR)')\n",
    "plt.legend(title='County', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0cbaf",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "INPUT_DIM = X_resampled_test.shape[1]\n",
    "NUM_CLASSES = int(len(le.classes_))  # 類別 0/1\n",
    "\n",
    "class BinaryMLP(nn.Module):\n",
    "    def __init__(self, in_dim=INPUT_DIM, num_classes=NUM_CLASSES, drop=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(drop),\n",
    "            nn.Linear(64, num_classes)  # logits\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995912e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nn, X_val_nn, y_train_nn, y_val_nn = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=42\n",
    ")\n",
    "\n",
    "X_train_t, y_train_t = to_tensors(X_train, y_train)\n",
    "X_val_t, y_val_t = to_tensors(X_val_nn, y_val_nn)\n",
    "X_test_t, y_test_t = to_tensors(X_resampled_test, y_resampled_test)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=256, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=512, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=512, shuffle=False)\n",
    "\n",
    "model = BinaryMLP().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val = -np.inf\n",
    "patience = 5\n",
    "wait = 0\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568f8c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    val_metrics = eval_loop(model, val_loader, le)\n",
    "    print(f'Epoch {epoch:02d}/{epochs} | loss {train_loss:.4f} | '\n",
    "          f'val_acc {val_metrics[\"acc\"]:.3f} | val_f1 {val_metrics[\"f1\"]:.3f} | val_auc {val_metrics[\"auc\"]:.3f}')\n",
    "\n",
    "    score_for_early = val_metrics[\"auc\"]  # 你也可用 f1\n",
    "    if score_for_early > best_val:\n",
    "        best_val = score_for_early\n",
    "        wait = 0\n",
    "        # torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= patience:\n",
    "            print('Early stopping.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994734e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryMLP().to(device)\n",
    "model.load_state_dict(torch.load('../ComputedData/ModelPerformance/nn_modelV3.pth'))\n",
    "\n",
    "test_metrics = eval_loop(model, test_loader, le)\n",
    "print(test_metrics['report'])\n",
    "\n",
    "# torch.save(model.state_dict(), '../ComputedData/ModelPerformance/nn_modelV3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = eval_loop(model, test_loader, le)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e401838",
   "metadata": {},
   "source": [
    "## Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cb6f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_model import build_groups_from_prefix, build_groups_with_interactions, build_pair_interaction_groups, PI_ML, PI_NN\n",
    "\n",
    "groups = build_groups_with_interactions(X_test.columns)\n",
    "\n",
    "print('lr')\n",
    "base_lr, perm_lr = PI_ML(lr, X_test, y_test, groups=groups, n_repeats=10)\n",
    "print('rf') \n",
    "base_rf, perm_rf = PI_ML(rf, X_test, y_test, groups=groups, n_repeats=10)\n",
    "print('nn')\n",
    "base_nn, perm_nn = PI_NN(model, X_test, y_test, groups=groups, n_repeats=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428cfecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perm_lr.to_csv('../ComputedData/Permutation/perm_lrV3.csv')\n",
    "# perm_rf.to_csv('../ComputedData/Permutation/perm_rfV3.csv')\n",
    "# perm_nn.to_csv('../ComputedData/Permutation/perm_nnV3.csv')\n",
    "\n",
    "perm_lr = pd.read_csv('../ComputedData/Permutation/perm_lrV3.csv')\n",
    "perm_rf = pd.read_csv('../ComputedData/Permutation/perm_rfV3.csv')\n",
    "perm_nn = pd.read_csv('../ComputedData/Permutation/perm_nnV3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27ce66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_translation = {\n",
    "    \"道路類別-第1當事者-名稱\": \"Road category – Party 1\",\n",
    "    \"速限-第1當事者\": \"Speed limit – Party 1\",\n",
    "    \"號誌-號誌種類名稱\": \"Traffic signal type\",\n",
    "    \"車道劃分設施-分向設施大類別名稱\": \"Lane division facility – directional separation (major category)\",\n",
    "    \"車輛撞擊部位大類別名稱-最初\": \"Vehicle impact location (major category, initial)\",\n",
    "    \"county\": \"County\",\n",
    "    \"車道劃分設施-分道設施-快車道或一般車道間名稱\": \"Lane Division Facility - Between Fast and General Lanes\",\n",
    "    \"車道劃分設施-分道設施-快慢車道間名稱\": \"Lane Division Facility - Between Fast and Slow Lanes\",\n",
    "    \"車道劃分設施-分道設施-路面邊線名稱\": \"Lane Division Facility - Road Edge Line\",\n",
    "    \"道路型態大類別名稱\": \"Road type (major category)\",\n",
    "    \"事故類型及型態大類別名稱\": \"Accident type and pattern (major category)\",\n",
    "    \"youbike\": \"YouBike\",\n",
    "    \"parkinglot\": \"Parking lot\",\n",
    "    \"道路障礙-視距名稱\": \"Road obstacle – Sight distance\",\n",
    "    \"路面狀況-路面狀態名稱\": \"Road surface condition – Surface status\",\n",
    "    \"mrt\": \"MRT\",\n",
    "    \"路面狀況-路面鋪裝名稱\": \"Road surface condition – Pavement type\",\n",
    "    \"道路障礙-視距品質名稱\": \"Road obstacle – Sight distance quality\",\n",
    "    \"道路障礙-障礙物名稱\": \"Road obstacle – Obstacle type\",\n",
    "    \"路面狀況-路面缺陷名稱\": \"Road surface condition – Defect type\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f7ac9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([\n",
    "    perm_lr.assign(model='LR'),\n",
    "    # perm_rf.assign(model='RF'),\n",
    "    # perm_nn.assign(model='NN')\n",
    "], ignore_index=True)\n",
    "\n",
    "combined[\"group\"] = combined[\"group\"].map(group_translation)\n",
    "\n",
    "order = (combined.groupby('group')['importance'].mean().sort_values(ascending=True).index.tolist())\n",
    "ypos = np.arange(len(order))\n",
    "\n",
    "triples = [\n",
    "    (perm_lr, 'lr'), \n",
    "    (perm_rf, 'rf'), \n",
    "    (perm_nn, 'nn')\n",
    "    ]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for perm_df_i, name in triples:\n",
    "    perm_df_i[\"group\"] = perm_df_i[\"group\"].map(group_translation)\n",
    "    d = (perm_df_i.set_index('group').reindex(order)) # 用統一群組順序對齊\n",
    "    plt.errorbar(\n",
    "        d['importance'],\n",
    "        (ypos),\n",
    "        xerr=d['std'],\n",
    "        fmt='o',\n",
    "        linewidth=2,\n",
    "        capsize=5,\n",
    "        label=name\n",
    "    )\n",
    "\n",
    "plt.yticks(ypos, order)\n",
    "plt.axvline(0.0, linestyle='--', linewidth=1)\n",
    "plt.xlabel('Permutation importance')\n",
    "plt.ylabel('Group')\n",
    "plt.title('Permutation importance (LR / RF / NN)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdf7ec",
   "metadata": {},
   "source": [
    "# Hitrate\n",
    "le的轉換是1為not hotspot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_cols = [col for col in X_resampled_test.columns if col.startswith('county_')]\n",
    "\n",
    "\n",
    "df_hitrate = X_resampled_test.copy()\n",
    "df_hitrate['y_true'] = y_resampled_test\n",
    "df_hitrate['y_pred'] = y_pred_lr\n",
    "\n",
    "hitrate = {}\n",
    "for col in county_cols:\n",
    "\n",
    "    mask = df_hitrate[df_hitrate[col] != False]\n",
    "    tn, fp, fn, tp = confusion_matrix(\n",
    "        mask['y_true'], mask['y_pred'], labels=[1, 0] # 這裡0是Hotspot\n",
    "    ).ravel()\n",
    "\n",
    "    # calculate precision, recall, accuracy, f1-score\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    hitrate[col] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "hitrate_df = pd.DataFrame.from_dict(hitrate, orient='index', columns=['precision', 'recall', 'accuracy', 'f1']).sort_values('f1', ascending=False)\n",
    "hitrate_df['county'] = hitrate_df.index\n",
    "hitrate_df['county'] = hitrate_df['county'].str.replace('county_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_model import hitrate_data\n",
    "# from config import countycity_dct\n",
    "\n",
    "hitrate_lr = hitrate_data(X_resampled_test, y_resampled_test, y_pred_lr)\n",
    "hitrate_rf = hitrate_data(X_resampled_test, y_resampled_test, y_pred_rf)\n",
    "hitrate_nn = hitrate_data(X_resampled_test, y_resampled_test, test_metrics['pred_y'])\n",
    "\n",
    "hitrate_lr['county'] = hitrate_lr['county'].map(countycity_dct)\n",
    "hitrate_rf['county'] = hitrate_rf['county'].map(countycity_dct)\n",
    "hitrate_nn['county'] = hitrate_nn['county'].map(countycity_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f500132b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'LR': hitrate_lr.copy(),\n",
    "    'RF': hitrate_rf.copy(),\n",
    "    'NN': hitrate_nn.copy(),\n",
    "}\n",
    "order = (results['LR'].sort_values('f1', ascending=False)['county']).tolist()\n",
    "\n",
    "metrics = ['precision', 'recall', 'accuracy', 'f1']\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "for i, met in enumerate(metrics, 1):\n",
    "    ax = plt.subplot(2, 2, i)\n",
    "    pos = np.arange(len(order))\n",
    "    width = 0.25\n",
    "    for j, (name, df) in enumerate(results.items()):\n",
    "        d = df.set_index('county').reindex(order)\n",
    "        ax.bar(pos + (j-1)*width, d[met].values, width=width, label=name)\n",
    "    ax.set_title(met)\n",
    "    ax.set_xticks(pos)\n",
    "    ax.set_xticklabels(order, rotation=45, ha='right')\n",
    "    if i == 2:\n",
    "        ax.legend(loc='upper right')\n",
    "    ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30cb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_bin(y_true, y_pred):\n",
    "    return {\n",
    "        'precision': precision_score(y_true, y_pred, pos_label=0),\n",
    "        'recall':    recall_score(y_true, y_pred, pos_label=0),\n",
    "        'f1':        f1_score(y_true, y_pred, pos_label=0),\n",
    "        'accuracy':  accuracy_score(y_true, y_pred),\n",
    "    }\n",
    "\n",
    "m_lr = metrics_bin(y_resampled_test, y_pred_lr)\n",
    "m_rf = metrics_bin(y_resampled_test, y_pred_rf)\n",
    "m_nn = metrics_bin(y_resampled_test, test_metrics['pred_y'])\n",
    "\n",
    "df = pd.DataFrame([m_lr, m_rf, m_nn], index=['LR','RF','NN'])\n",
    "metrics = ['precision', 'recall', 'f1', 'accuracy']\n",
    "df = df[metrics]\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.25\n",
    "\n",
    "for i, model in enumerate(df.index):\n",
    "    plt.bar(x + (i-1)*width, df.loc[model].values, width=width, label=model)\n",
    "\n",
    "plt.xticks(x, metrics)\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Overall Metrics on Resampled Test')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "for i, model in enumerate(df.index):\n",
    "    vals = df.loc[model].values\n",
    "    for xi, v in zip(x + (i-1)*width, vals):\n",
    "        plt.text(xi, v + 0.01, f'{v:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ST-RTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
