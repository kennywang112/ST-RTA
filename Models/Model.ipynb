{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb03cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "analyze_path = os.path.join(parent_dir, \"utils\")\n",
    "\n",
    "os.chdir(analyze_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1b50e",
   "metadata": {},
   "source": [
    "要先建立輸入到模型的資料\n",
    "- 若是要分類是否是熱點，應該要以一個區域的grid為單位\n",
    "- 所以建立得grid亦包含該地區的所有特徵資料，以比例顯示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2ccdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "from utils import get_grid, calculate_gi\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475cd44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataA1 = pd.read_csv('../ComputedData/Accident/DataA1_with_MYP.csv')\n",
    "dataA2 = pd.read_csv('../ComputedData/Accident/DataA2_with_MYP.csv')\n",
    "\n",
    "filtered_A2 = dataA2[dataA2['當事者順位'] == 1]\n",
    "filtered_A1 = dataA1[dataA1['當事者順位'] == 1]\n",
    "\n",
    "filtered_A1['source'] = 'A1'\n",
    "filtered_A2['source'] = 'A2'\n",
    "filtered_A1['num_accidents'] = 1 \n",
    "filtered_A2['num_accidents'] = 1\n",
    "combined_data = pd.concat([filtered_A1, filtered_A2], ignore_index=True)\n",
    "\n",
    "# hex_grid = get_grid(combined_data, hex_size=0.01, threshold=-1)\n",
    "# taiwan = taiwan.to_crs(hex_grid.crs)\n",
    "\n",
    "# hex_grid = hex_grid[hex_grid.intersects(taiwan.unary_union)]\n",
    "# hex_grid.to_csv('../ComputedData/Grid/hex_grid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e19c72a",
   "metadata": {},
   "source": [
    "## obtain hotspot's county"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89cd171",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely import wkt\n",
    "import geopandas as gpd\n",
    "import ast\n",
    "\n",
    "taiwan = gpd.read_file('../Data/OFiles_9e222fea-bafb-4436-9b17-10921abc6ef2/TOWN_MOI_1140318.shp')\n",
    "taiwan = taiwan[(~taiwan['TOWNNAME'].isin(['旗津區', '頭城鎮', '蘭嶼鄉', '綠島鄉', '琉球鄉'])) & \n",
    "                (~taiwan['COUNTYNAME'].isin(['金門縣', '連江縣', '澎湖縣']))]\n",
    "\n",
    "TM2 = 3826\n",
    "hex_grid_raw = pd.read_csv('../ComputedData/Grid/hex_grid.csv')\n",
    "hex_grid_raw['geometry'] = hex_grid_raw['geometry'].apply(wkt.loads)\n",
    "hex_grid = gpd.GeoDataFrame(hex_grid_raw, geometry='geometry').set_crs(TM2, allow_override=True)\n",
    "\n",
    "grid_gi_df = pd.read_csv('../ComputedData/Grid/grid_gi.csv')\n",
    "grid_gi_df['accident_indices'] = grid_gi_df['accident_indices'].apply(ast.literal_eval)\n",
    "grid_gi_df['geometry'] = grid_gi_df['geometry'].apply(wkt.loads)\n",
    "grid_gi  = gpd.GeoDataFrame(grid_gi_df, geometry='geometry').set_crs(TM2, allow_override=True)\n",
    "\n",
    "taiwan_tm2 = taiwan.to_crs(TM2)\n",
    "\n",
    "taiwan_cnty = taiwan_tm2[['COUNTYNAME','geometry']].dissolve(by='COUNTYNAME')\n",
    "taiwan_cnty['geometry'] = taiwan_cnty.buffer(0)\n",
    "taiwan_cnty = taiwan_cnty.reset_index()\n",
    "\n",
    "pts = hex_grid.copy()\n",
    "pts['geometry'] = pts.geometry.centroid\n",
    "\n",
    "county_join = gpd.sjoin(\n",
    "    pts[['geometry']], taiwan_cnty, how='left', predicate='within'\n",
    ")[['COUNTYNAME']]\n",
    "\n",
    "print('NaN ratio:', county_join['COUNTYNAME'].isna().mean())\n",
    "county_join.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2f540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_gi['COUNTYNAME'] = county_join['COUNTYNAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef8ae0",
   "metadata": {},
   "source": [
    "hex_grid目前包含所有事故索引，所以要回推\n",
    "- 回推方式從combined_data獲取，並且計算他們的事故特徵平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ec975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 會造成重複的資料，因為grid可能覆蓋多個縣市\n",
    "# grid = calculate_gi(6, hex_grid, adjacency='knn')\n",
    "# grid = gpd.sjoin(hex_grid, taiwan[['COUNTYNAME', 'geometry']], how='left', predicate='intersects')\n",
    "# grid.to_csv('../ComputedData/Grid/grid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20b7023",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_group = [\n",
    "    # 氣候暫不討論\n",
    "    # '天候名稱', '光線名稱',\n",
    "\n",
    "    # 道路問題\n",
    "    '路面狀況-路面鋪裝名稱', '路面狀況-路面狀態名稱', '路面狀況-路面缺陷名稱',\n",
    "    '道路障礙-障礙物名稱', '道路障礙-視距品質名稱', '道路障礙-視距名稱',\n",
    "\n",
    "    # 號誌\n",
    "    '號誌-號誌種類名稱', '號誌-號誌動作名稱',\n",
    "\n",
    "    # 車道劃分\n",
    "    '車道劃分設施-分道設施-快車道或一般車道間名稱',\n",
    "    '車道劃分設施-分道設施-快慢車道間名稱', '車道劃分設施-分道設施-路面邊線名稱',\n",
    "\n",
    "    # 大類別\n",
    "    # '肇因研判大類別名稱-主要', '肇因研判大類別名稱-個別', # 聚焦道路類型\n",
    "    # '當事者區分-類別-大類別名稱-車種', # 聚焦道路類型\n",
    "    # '當事者行動狀態大類別名稱', # 聚焦道路類型\n",
    "    '車輛撞擊部位大類別名稱-最初', #'車輛撞擊部位大類別名稱-其他',\n",
    "    '事故類型及型態大類別名稱', '車道劃分設施-分向設施大類別名稱',\n",
    "    '事故位置大類別名稱', '道路型態大類別名稱',\n",
    "    \n",
    "    # 子類別\n",
    "    # '肇因研判子類別名稱-主要', '肇因研判子類別名稱-個別', # 聚焦道路類型\n",
    "    # '當事者區分-類別-子類別名稱-車種', # 聚焦道路類型\n",
    "    # '當事者行動狀態子類別名稱', # 聚焦道路類型\n",
    "    # '車輛撞擊部位子類別名稱-最初', '車輛撞擊部位子類別名稱-其他', # 道路類型很大程度影響撞擊部位，所以不考慮\n",
    "    # '事故類型及型態子類別名稱', '車道劃分設施-分向設施子類別名稱', \n",
    "    # '事故位置子類別名稱', '道路型態子類別名稱',\n",
    "\n",
    "    # 其他\n",
    "    # '當事者屬-性-別名稱', '當事者事故發生時年齡', \n",
    "    '速限-第1當事者', '道路類別-第1當事者-名稱',\n",
    "    # '保護裝備名稱', '行動電話或電腦或其他相類功能裝置名稱', '肇事逃逸類別名稱-是否肇逃',\n",
    "\n",
    "    # 設施\n",
    "    'youbike_100m_count', 'mrt_100m_count', 'parkinglot_100m_count',\n",
    "\n",
    "    # A1 or A2\n",
    "    # 'source',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cf4ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(\n",
    "        grid, combined_data, select_group, rows\n",
    "        ):\n",
    "\n",
    "    indices = grid['accident_indices'].iloc[rows] # return list of original data index\n",
    "    sample = combined_data.iloc[indices]\n",
    "    sample = sample[select_group]\n",
    "\n",
    "    cat_cols = sample.select_dtypes(include='object').columns\n",
    "    num_cols = sample.select_dtypes(include='number').columns\n",
    "\n",
    "    cat_features = []\n",
    "    num_features = []\n",
    "    # for categorical features\n",
    "    if len(cat_cols) > 0:\n",
    "        for col in cat_cols:\n",
    "            vc = sample[col].value_counts(normalize=True)\n",
    "            vc.index = [f\"{col}_{v}\" for v in vc.index]\n",
    "            cat_features.append(vc)\n",
    "        cat_features = pd.concat(cat_features)\n",
    "    else:\n",
    "        cat_features = pd.Series(dtype='float64')\n",
    "    # for numerical features\n",
    "    if len(num_cols) > 0:\n",
    "        num_features = sample[num_cols].mean()\n",
    "        num_features.index = [f\"{col}_mean\" for col in num_features.index]\n",
    "    else:\n",
    "        num_features = pd.Series(dtype='float64')\n",
    "\n",
    "    all_features = pd.concat([cat_features, num_features])\n",
    "    all_features_df = all_features.to_frame().T\n",
    "\n",
    "    return all_features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec022585",
   "metadata": {},
   "source": [
    "## Features concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafa5aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features_list = []\n",
    "\n",
    "grid_filter = grid_gi[grid_gi['accident_indices'].str.len() > 0]\n",
    "for rows in range(grid_filter.shape[0]):\n",
    "    features = extract_features(grid_filter, combined_data, select_group, rows)\n",
    "    all_features_list.append(features)\n",
    "\n",
    "all_features_df = pd.concat(all_features_list, ignore_index=True)\n",
    "all_features_df.fillna(0, inplace=True)\n",
    "\n",
    "all_features_df[['mrt_100m_count_mean', 'youbike_100m_count_mean', 'parkinglot_100m_count_mean', '速限-第1當事者_mean']] =\\\n",
    "      all_features_df[['mrt_100m_count_mean', 'youbike_100m_count_mean', 'parkinglot_100m_count_mean', '速限-第1當事者_mean']].\\\n",
    "        apply(lambda x: (x - x.min()) / (x.max() - x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf09ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_grid = pd.concat([grid.reset_index(drop=True)[['hotspot', 'COUNTYNAME']], all_features_df], axis=1)\n",
    "# new_grid.to_csv(\"../ComputedData/ForTDA/new_grid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96d0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with county town\n",
    "# 原始資料index並非從1開始所以需reset\n",
    "new_grid = pd.concat([grid_filter.reset_index(drop=True)[['hotspot', 'COUNTYNAME']], all_features_df], axis=1)\n",
    "county_dummies = pd.get_dummies(new_grid['COUNTYNAME'], prefix='county')\n",
    "new_grid_encoded = pd.concat([new_grid.drop(['COUNTYNAME'], axis=1), county_dummies], axis=1)\n",
    "\n",
    "# without county town\n",
    "# new_grid = pd.concat([grid_filter.reset_index(drop=True)[['hotspot']], all_features_df], axis=1)\n",
    "# new_grid_encoded = new_grid\n",
    "\n",
    "# binary hotspot\n",
    "new_grid_encoded['hotspot'] = new_grid_encoded['hotspot'].apply(lambda x: 'Hotspot' if 'Hotspot' in str(x) else 'Not Hotspot')\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(new_grid_encoded['hotspot'])\n",
    "X = new_grid_encoded.drop(columns=['hotspot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d53e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "y_train = pd.Series(y_train, index=X_train.index)\n",
    "y_test  = pd.Series(y_test,  index=X_test.index)\n",
    "\n",
    "# with undersampling\n",
    "cls_counts = y_test.value_counts()\n",
    "min_count = cls_counts.min()\n",
    "rus_test = RandomUnderSampler(\n",
    "    sampling_strategy={int(c): int(min_count) for c in cls_counts.index},\n",
    "    random_state=42\n",
    ")\n",
    "X_resampled_test, y_resampled_test = rus_test.fit_resample(X_test, y_test)\n",
    "\n",
    "# without undersampling\n",
    "# X_resampled_test = X_test.copy()\n",
    "# y_resampled_test = y_test.copy()\n",
    "\n",
    "print(\"before US\")\n",
    "print(pd.Series(y_test).map(dict(enumerate(le.classes_))).value_counts())\n",
    "print(\"after US\")\n",
    "print(pd.Series(y_resampled_test).map(dict(enumerate(le.classes_))).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0b4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "        class_weight='balanced', max_iter=1000, \n",
    "        random_state=42, \n",
    "        multi_class='multinomial'\n",
    "    )\n",
    "rf = RandomForestClassifier(\n",
    "        n_estimators=300, max_depth=None, min_samples_leaf=1,\n",
    "        class_weight='balanced', n_jobs=-1, random_state=42,\n",
    "    )\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, clf in [('Logistic', lr), ('RandomForest', rf)]:\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=cv, n_jobs=-1,\n",
    "                            scoring='roc_auc_ovr_weighted',\n",
    "                            # scoring='roc_auc'\n",
    "                             )\n",
    "    print(f'{name} CV ROC AUC: {scores.mean():.3f} ± {scores.std():.3f}')\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "proba_test_lr = lr.predict_proba(X_resampled_test)\n",
    "proba_test_rf = rf.predict_proba(X_resampled_test)\n",
    "y_pred_lr = np.argmax(proba_test_lr, axis=1)\n",
    "y_pred_rf = np.argmax(proba_test_rf, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c85676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "y_pred = y_pred_rf\n",
    "proba_test = proba_test_rf\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_resampled_test, y_pred, labels=range(len(le.classes_))))\n",
    "\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(\n",
    "    y_resampled_test, y_pred, target_names=le.classes_, digits=3\n",
    "))\n",
    "\n",
    "\n",
    "if proba_test.shape[1] == 2:\n",
    "    # 二元分類\n",
    "    roc_auc = roc_auc_score(y_resampled_test, proba_test[:, 1])\n",
    "    print(f'ROC AUC: {roc_auc:.3f}')\n",
    "    y_test_bin = label_binarize(y_resampled_test, classes=range(len(le.classes_)))\n",
    "    pr_auc_macro  = average_precision_score(y_test_bin, proba_test[:, 1], average='macro')\n",
    "    pr_auc_weight = average_precision_score(y_test_bin, proba_test[:, 1], average='weighted')\n",
    "    print(f'PR  AUC macro: {pr_auc_macro:.3f}')\n",
    "    print(f'PR  AUC wighted: {pr_auc_weight:.3f}')\n",
    "else:\n",
    "    # 多類分類\n",
    "    roc_auc = roc_auc_score(y_resampled_test, proba_test, average='weighted', multi_class='ovr')\n",
    "    print(f'ROC AUC: {roc_auc:.3f}')\n",
    "    # 多類PR AUC需要 binarize 後用 one-vs-rest，再做 macro/weighted 平均\n",
    "    y_test_bin = label_binarize(y_resampled_test, classes=range(len(le.classes_)))  # shape [n, n_classes]\n",
    "    pr_auc_macro  = average_precision_score(y_test_bin, proba_test, average='macro')\n",
    "    pr_auc_weight = average_precision_score(y_test_bin, proba_test, average='weighted')\n",
    "    print(f'PR  AUC macro: {pr_auc_macro:.3f}')\n",
    "    print(f'PR  AUC wighted: {pr_auc_weight:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6017ce8",
   "metadata": {},
   "source": [
    "設施平均：該地區的事故點附近平均會有幾個設施"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0bbf5",
   "metadata": {},
   "source": [
    "# RandomForest Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4a6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = X_train.columns\n",
    "\n",
    "for i in indices:\n",
    "    print(f\"{feature_names[i]}: {importances[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5265a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X_train.columns\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "fi_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "fi_df['main_feature'] = fi_df['feature'].str.split('_').str[0]\n",
    "grouped = fi_df.groupby('main_feature')['importance'].sum().sort_values(ascending=False)\n",
    "\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99b80c",
   "metadata": {},
   "source": [
    "# LinearRegression coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d679d661",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = lr.coef_[0]  # 若是二元分類，取第0個類別\n",
    "feature_names = X_train.columns\n",
    "indices = np.argsort(np.abs(coefs))[::-1]  # 依照絕對值排序\n",
    "\n",
    "for i in indices:\n",
    "    print(f\"{feature_names[i]}: {coefs[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e401838",
   "metadata": {},
   "source": [
    "## Permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "def build_groups_from_prefix(columns, sep=\"_\"):\n",
    "    groups = {}\n",
    "    for c in columns:\n",
    "        prefix = c.split(sep, 1)[0]\n",
    "        groups.setdefault(prefix, []).append(c)\n",
    "    return groups\n",
    "\n",
    "def grouped_permutation_importance(model, X_df, y, groups=None, n_repeats=5, random_state=42):\n",
    "    X_arr = X_df.to_numpy(copy=True)\n",
    "    name_to_idx = {c: i for i, c in enumerate(X_df.columns)}\n",
    "    if groups is None:\n",
    "        idx_groups = {c: [i] for i, c in enumerate(X_df.columns)}\n",
    "    else:\n",
    "        idx_groups = {g: [name_to_idx[c] for c in cols] for g, cols in groups.items()}\n",
    "\n",
    "    def get_ap(proba):\n",
    "        # 二元：取正類；多類：取第二欄 (假設 label encoder 對應)；視需求調整\n",
    "        if proba.ndim == 2 and proba.shape[1] > 1:\n",
    "            target_scores = proba[:, 1]\n",
    "        else:\n",
    "            target_scores = proba\n",
    "        return average_precision_score(y, target_scores)\n",
    "\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    base = get_ap(model.predict_proba(X_arr))\n",
    "    n_samples = X_arr.shape[0]\n",
    "    X_work = X_arr.copy()\n",
    "\n",
    "    rows = []\n",
    "    for gname, cols in idx_groups.items():\n",
    "        original = X_work[:, cols].copy()\n",
    "        losses = []\n",
    "        for _ in range(n_repeats):\n",
    "            perm_idx = rng.permutation(n_samples)\n",
    "            X_work[:, cols] = original[perm_idx, :]\n",
    "            score = get_ap(model.predict_proba(X_work))\n",
    "            losses.append(base - score)\n",
    "            X_work[:, cols] = original  # restore\n",
    "        rows.append((gname, float(np.mean(losses)), float(np.std(losses))))\n",
    "\n",
    "    out = (pd.DataFrame(rows, columns=[\"group\", \"importance\", \"std\"])\\\n",
    "             .sort_values(\"importance\", ascending=False)\\\n",
    "             .reset_index(drop=True))\n",
    "    return base, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = build_groups_from_prefix(X_test.columns)\n",
    "base_ap, gperm = grouped_permutation_importance(rf, X_test, y_test, groups=groups, n_repeats=10)\n",
    "print('Base AP:', base_ap)\n",
    "print(gperm.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdf7ec",
   "metadata": {},
   "source": [
    "# Hitrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f44060",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hitrate = X_resampled_test.copy()\n",
    "df_hitrate['y_true'] = y_resampled_test\n",
    "df_hitrate['y_pred'] = y_pred_rf\n",
    "\n",
    "hitrate = {}\n",
    "for col in county_cols:\n",
    "    mask = df_hitrate[df_hitrate[col] != False]\n",
    "    if mask.shape[0] > 0:\n",
    "        hitrate[col] = (mask['y_true'] == mask['y_pred']).mean()\n",
    "    else:\n",
    "        hitrate[col] = None\n",
    "\n",
    "hitrate_df = pd.DataFrame.from_dict(hitrate, orient='index', columns=['hitrate']).sort_values('hitrate', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "hitrate = dict(sorted(hitrate.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "for k, v in hitrate.items():\n",
    "    mask = df_hitrate[df_hitrate[k] != False]\n",
    "    print(f\"county: {k}, hit rate: {v:.4f}, data points: {mask.shape[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ST-RTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
